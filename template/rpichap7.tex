%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                                                 %
%                            CHAPTER SEVEN                        %
%                                                                 %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{DISCUSSION}

\section{Model}

The resulting model addresses versioning by looking at the attributes of each version.
Other ontologies takes a higher level view in terms of version modeling.
While it is more specific, this implementation forces some space requirements.
PROV only requires 3 to 5 triples in order to make a versioning statement.
This model uses 9 triples for a mod change and 7 to encode addition and invalidation.
To model a version has space complexity of \(O(7M+5(A+I))\) since the version declaration statements overlap.
However, a similar structure can be achieved using prov:wasDerivedFrom to replace modifications and schema:AddAction and schema:DeleteAction to replace additions and invalidations.
The resulting space complexity is \(O(7M+3A+5I)\).
This is fairly similar with additions seeing a reduction since the left-hand version no longer contributes to the AddAction.
Thus the primary benefit of using this model comes from semantics.

The reason prov:Generation and prov:Invalidation are not used is because they expect an activity to be responsible for an object.
However, it is not generally true that an action actively added or removed an attribute from an object in the left-hand version to produce the right-hand revision.
That mentality denies the ability to conduct versioning comparisons between objects that are not sequentially adjacent.
The activity producing a far away object may not immediately relate to the original data in a version comparison, resulting in a situation where it would be inappropriate to use the two PROV concepts.
When considering versioning in a state-based sense, relationships exist as a result of two objects being versions of each other.

\section{Implementation}

The versioning process breaks down to three formal steps which appear in all contexts of versioning studied in this thesis.
The first activity verifies that the objects being compared are actually versions of each other.
This exercise is often left out of details in practice since a data producer is often fairly certain as to the state of their versions.
Mechanisms are otherwise employed to enforce a strict documentation procedure to ensure the data's comparability as seen in version control software.
However, this step establishes the foundation and validity of further actions taken to version the objects.
This ensures that a mapping can be performed and will be meaningful.
The next step is generating the mapping to identify addition, invalidation, and modification relationships.
The resultant mapping in spreadsheet comparisons followed very similar rules, but when looking at the MBVL dataset, the definitions were changed to achieve a specific goal.
The final step involves publishing the change information using the mapping.
In this thesis, the resulting product is published into a versioning graph.

One of the desired contributions was to study the possibility of a machine readable change log.
In this implementation, the number of triples necessary to implement the model significantly impairs the log's ability to remain human readable.
The Noble Gas data set's change logs could not be loaded using a web browser.
One contributor to this problem is that the modification of an entire column would result in multiple entries equal to the number of rows in the table.
These entries could be combined together into a single statement relating just the effected columns.
However, this optimization would greatly impact the resulting change counts, reinforcing that version analysis depends largely on the mapping method, but this would likely allow the log to become readable.
JSON-LD proves to be a better mechanism for encoding the versioning graph than RDFa since it is intended to encode data while the latter primarily contextualizes visible content.

When linking together multiple versions using a versioning graph, the relationship between non-adjacent editions remains implied in the graph's structure.
The natural pathway between attributes in non-adjacent versions holistically considers the relationships among all attributes along that path.
In comparison, other models only capture activity between the adjacent versions.

\section{Version Identification}

Version identifiers provide a very important role in communicating change information because users see it first.
However, their application is subjective as can be seen in the case of GCMD Keywords.
The producers created a change in their data set which they did not report in their change log or capture in their identifier.
By connecting indicators to change measures such as the versioning model of this thesis, they better convey the change within a data set.
This is not to say that change counts should always be used since a single modification can have greater implications.
However, a poor understanding in the amount of change between two versions can lead to flawed expectations in migrating across them.
This can lead to extra or unexpected costs in using the newer data set.

Another thing to consider is that counters in magnitudes by themselves may not characterize data set change well.
In Figure \ref{GCMDC2}, the yellow line indicates the total changes made to the data set, performing a similar function as the major/minor/revision version identifier.
However, breaking up the changes into types reveals the dominant contribution of additions to the data set.
This understanding of the data's behavior cannot be revealed with a three number dot decimal identifier system.

\section{Change Analysis}

In Chapter \ref{ch:mbvl}, the versioning process was used to compare the performance of different taxonomy and algorithm combinations.
This shows that versioning methods go beyond standard documentation goals, and provide a method to record decision justifications.
Clever use of the mapping procedure allows the analysis to be performed.