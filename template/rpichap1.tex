%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                                                 %
%                            CHAPTER ONE                          %
%                                                                 %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{INTRODUCTION}

\section{Why Versioning is Important}

\begin{quotation}
	``If scientific data production were easy, instruments would
	have stable calibrations and validation activities would discover no need for
	corrections that vary with time. Unfortunately, validation invariably shows that
	instrument calibrations drift and that algorithms need a better physical basis." \cite{Barkstrom2003}
\end{quotation}

Anyone who has used an iPhone or owned a video game console understands the basics of versioning.
Companies brand sequential devices to indicate improvements in performance or capabilities.
Basic numerical sequencing has given rise to a plethora of versioning systems used widely across a landscape of software and data.
Versioning systems help scientific workflows avoid losing work by managing transitions and changes while in operation \cite{Casati1996}.
Versioning systems provide necessary documentation which informs the transition to new methods and procedures \cite{Wiil:2000:RDH:338407.338517}.
Versioning systems provide accountability for the value of a project's data set when considering an agency's continued funding \cite{Cavanaugh2002}.
The natural evolution of versioning systems, however, have given rise to formal architecture operating on top of very informal concepts.
In this dissertation, we identify gaps in versioning practices which result from tradition and develop a data model to more completely capture the interactions involved in versioning.

We know that our instruments are imperfect, thus making our data also imperfect.
The data, however, can be collected under quantifiable amounts of error for which makes the data still usable.
When imperfections within the data exceed expectations, versioning systems allow exceptional errors to be managed.
The data can be removed, corrected, or specially treated to bring the data error back within expectations.

At the very core, versioning systems are a means of communication.
Data producers communicate to consumers how much the producer has changed the data.
The producer can also communicate through time using logs and other documentation.
In order to clearly communicate changes, ideas must be clearly formalized and defined.

\section{Definitions of Version}

Using the term `versions' in the vernacular has become so pervasive that few documents formally define it.
Barkstrom describes versions as \textbf{homogeneous groupings} used to control, ``production volatility induced by changes in algorithms and coefficients as result of validation and reprocessing," \cite{Barkstrom2003}.
The \textbf{groupings} he mentions is a method of separating data objects such that they have similar scientific or technical properties.
In order to determine when these properties have changed, he leverages the NASA Earth Science workflow model shown in Figure \ref{NASALevels}.
\begin{figure}
	\centering
	\includegraphics[scale=0.35]{figures/NASALevels.png}
	\caption[National Aeronautics and Space Administration Earth Science organizes its data into three levels depending on the amount of aggregation and the distance the data is removed from the original sensor measurements.]{National Aeronatuics Space Administration Earth Science organizes its data into three levels depending on the amount of aggregation and the distance the data is removed from the original sensor measurements. Figure 1 from \cite{Barkstrom2003}}
	\label{NASALevels}
\end{figure}
The model describes the formal stages of processing to turn a raw remote sensing signal from satellite instruments into global aggregate summaries \cite{Barkstrom2003}.
Understanding this model reveals that changes to either the algorithms or parameter files will force a change in the resulting data, creating a new version of the output data.
Essentially, versions are a means to communicate how much data has diverged as a result of changes to an object's provenance.

Another definition comes from Tagger in which versions are a, ``semantically meaningful snapshot of a design object," \cite{Tagger2005}.
He, unfortunately, does not further clarify what he means by semantically meaningful.
The design object unifies the versions as their primary subject, capturing the object's state over the course of its design.

The \textit{derivation}, PROV Ontology's analog for a version and covered more in Section \ref{sec:prov}, is defined as, ``a transformation of an entity into another, an update of an entity resulting in a new one, or the construction of a new entity based on a pre-existing entity," \cite{Lebo2013}.
In this view, a \textbf{version} exists in comparison to another object.

The Functional Requirements for Bibliographical Records (FRBR) avoids the terms \textbf{edition} and \textbf{version} since ``those terms are neither clearly defined nor uniformly applied" \cite{frbr}.
Instead, they use the terms: \textbf{work}, \textbf{expression}, and \textbf{manifestation}.
A \textbf{work} refers to the abstract concept of a creative or artistic idea.
\textbf{Expressions} are then different forms of that particular \textbf{work}, embodying the most similar term to the previous definitions of versions.
A \textbf{manifestation} is the physical embodiment of an \textbf{expression}.
These three terms and their hierarchy establish a repeating theme throughout other versioning works.

Combining these myriad of definitions together, a version is ``an \textbf{expression} of a \textbf{work} which exists in comparison to another object and communicates the extent to which it diverges from that object as a result of provenance changes".
Although each definition disagrees on the form a version object takes on, all but PROV derivation agree that a version belongs to a larger collection of objects implementing a more abstract, ideal representation.
Provenance provides the information necessary to explain semantically meaningful for the Tagger definition as \textit{prov:Derivation} captures when a data object diverges into a new object.


\section{Version Models} \label{sec:models}

Version models provide a visual theoretical aid in understanding where a data object lies in relation to the rest of a work.
The Atmospheric Radiation Measurements (ARM) group used a model dividing the data into mathematical sets which versioning operations acted upon\cite{6906868}.
Adding files already in the set created a new set which inherited all non-intersecting files and included all the new ones.
The model provided a means to organize and automate the versioning of ARM's daily expanding data sets.

The Health Care and Life Sciences (HCLS) Interest Group of the World Wide Web Consortium (W3C) recently released a model which may provide a solution when used in conjunction with other identifiers \cite{Dummontier2016}.
Their model, shown in Figure \ref{HCLSModel}, separates the concept of a data set into three groupings.
\begin{figure}%[b]
	\centering
	\includegraphics[scale=0.34]{figures/HCLSModel.png}
	\caption[Data model from the Health Care and Life Sciences Interest Group separating data into three levels: works, versions, and instances.]{Data model from the Health Care and Life Sciences Interest Group separating data into three levels: works, versions, and instances.  From Dummontier, et al. \cite{Dummontier2016}}
	\label{HCLSModel}
\end{figure}
The highest level summarizes the data as an abstract work, perhaps better described as a topic or title.
The data topic can have multiple versions over time.
The version can then be instantiated into various distributions with different physical formats.
The model---relating summary, version, and distribution---also strongly resembles the formation of FRBR's work, expression, and manifestation model.

\begin{figure}
	\centering
	\includegraphics[scale=0.50]{figures/hierarchy.png}
	\caption[Visual representation of grouping hierarchy.]{Visual representation of grouping hierarchy.  From \cite{Barkstrom2003}}
	\label{hierarchy}
\end{figure}

From his definition of versions, Barkstrom also outlines an hierarchical version model as seen in Figure \ref{hierarchy}.
The model features additional intermediary levels than the HCLS's model, following NASA's data curation practices \cite{barkstrom2014earth}.
Each edge in the tree signifies a difference with other objects at the same depth, but the model does not provide a mechanism to explain the difference.
The difference in the number of tiers employed in the HCLS and Barkstrom models also indicates that different applications will have varying expectations of granularity to their versioning models.
A general solution will likely need to be tiered and recursive in structure to accommodate different levels of specificity.

\section{Provenance Representation}

Provenance ontologies form a major section of linked data approaches to data versioning.
The coverage stems from the close relation between provenance and differentiating versions.
The Proof Markup Language, one of the first semantic models to capture provenance information, expressed lineage relationships using inference reasoning through traceable graphs \cite{daSilva2006381}.
The technique provides a powerful way to express and imply sequences of relationships between different versions and characterize the manner of their relation.

\subsection{Open Provenance Model}

A number of linked data models include versioning concepts such as the Open Provenance Model (OPM) \cite{moreau2008open}.
Driven by the uncertain needs and sometimes conflicting conventions of different scientific domains, the model sought to find a method to standardize the way in which provenance data is captured while also keeping the specification open to accommodate current data sets through the change.
In an experimental case, the model has been applied to sensor networks, automating and unifying their provenance capture even as they grow \cite{5478496}.
To aid OPM's adoption, the framework Karma2 integrates provenance capture into scientific workflows and provides a more abstract view of their data collection activities \cite{simmhan2010karma2}.
The property \textit{opm:WasDerivedFrom} constitutes a core concept in the model and marks the reliance of one object's existence on another object.
For a large part, this encompasses the engagement which provenance models view versions, without further need to explore the derivation's content.

\subsection{PROV-O}\label{sec:prov}

PROV, a World Wide Web Consortium (W3C) Recommendation, delineates a method to express data provenance in a more compact form as seen in Figure \ref{PROVO} \cite{Gil2013a} \cite{Groth2013}.
\begin{figure}
	\centering
	\includegraphics[scale=0.5]{figures/ProvO.png}
	\caption[Diagram of the PROV Ontology.]{Diagram of the PROV Ontology.  Figure 1 from \cite{Lebo2013}}
	\label{PROVO}
\end{figure}
The recommendation uses a conceptual model relating activities, agents, and entities to describe data production lineage \cite{Moreau2013c} \cite{Nies2013} \cite{Nies2013a}.
Intended as a high level abstraction, it takes an activity-oriented approach to provenance modeling.
Every data entity results from the actions of some activity \cite{Gil2013}.
The conceptual model's expression occurs through the PROV Ontology (PROV-O), which can be conveyed through various resource description languages \cite{Hua2013} \cite{Klyne2013}.
The ontology is further formalized into a functional notation for easier human consumption \cite{Moreau2013b} \cite{Cheney2013a}.
One particular strength that has contributed to the adoption of PROV is its ability to link into other ontologies, making it easier for existing semantically enriched data sets to adopt PROV \cite{Miles2013} \cite{Moreau2013}.

PROV has provided a major contribution in maintaining the quality and reproducibility of data sets and reporting in the National Climate Assessment (NCA) \cite{Ma2014191}.
The contribution signifies that there is an increased likelihood of adoption through other scientific fields as a result of this reporting.
The Global Change Information System, which houses the data used to generate the NCA, uses PROV to meticulously track the generation of its artifacts and results as they are used in assessment report \cite{Tilmes2012}.
The usage means that not only does the data have a traceable lineage to verify quality, but the content of documents can have the same verifiability \cite{Ma2014}.
Komadu, a framework developed to alleviate workflow integration, utilizes PROV to improve upon its predecessor, Karma, by no longer utilizing global context identifiers that were not necessarily shared throughout the workflow. \cite{Suriarachchi_2015}.

The PROV Ontology provides three different concepts that begin to encapsulate the provenance relationship between data versions.
It defines a \textit{prov:Generation} as "the completion of production of a new entity by an activity," \cite{Lebo2013}.
This means that the generation, which corresponds adding an object to a version, must result from a \textit{prov:Activity}.
\textit{Prov:Invalidation}, defined as the, ``start of the destruction, cessation, or expiry of an existing entity by an activity," makes a similar connection between activities and entities \cite{Lebo2013}.
A third concept, \textit{prov:Derivation}, relates two entities, and the ontology defines it as, "a transformation of an entity into another, an update of an entity resulting in a new one, or the construction of a new entity based on a preexisting entity. " \cite{Lebo2013}.
PROV also has a property called \textit{prov:isDerivedFrom} which conveys the same definition as a \textit{prov:Derivation}.
Using the property and concept together forms a qualified property which can be instantiated and further annotated.

\subsection{Provenance, Authorship, and Versioning Ontology}

The Provenance, Authorship, and Versioning (PAV) Ontology is, ``a lightwei\-ght vocabulary, for capturing ``just enough" descriptions essential for web resources representing digitized knowledge" \cite{Ciccarese2013}.
It provides a means to track versioning information through linked data by introducing \textit{pav:version} to cite versions and \textit{pav:previousVersion} to link them together in order \cite{Ciccarese2013}.
It does so in comparison to the Dublin Core concept \textit{dc:isVersionOf} which records, "Changes in version imply substantive changes in content rather than differences in format" \cite{DCMI2012}.
PAV supports the idea that a new concept becomes necessary to cover cases where new versions do not have to be substantive but can still be alternate editions of the original object.
While it documents related versions well, PAV does not dive deeper in explaining the circumstances behind version differences.

\subsection{Schema.org}

\sloppy
The Schema.org ontology is not a provenance ontology but provides a means to supply searchable web pages with standardized micro-data.
The ontology has a collection of concepts which could be applied to versioning.
The \textit{schema:UpdateAction} is defined as, ``the act of managing by changing/editing the state of the object," which encompasses the same responsibilities expected of versioning systems \cite{Schema}.
The terms \textit{schema:AddAction}, \textit{schema:DeleteAction}, and \textit{schema:ReplaceAction} subclass the \textit{shcema:UpdateAction}.
These classes model actions which further cement parallels between versioning and \textit{schema:UpdateAction}.

\fussy
Schema.org defines a \textit{schema:ReplaceAction} as, ``the act of editing a recipient by replacing an old object with a new object" \cite{SchemaRep}.
The concept has two properties, \textit{schema:replacee} and \textit{schema:replacer} which indicates that a new object replaces an old one.
Schema.org models the interaction by placing the replacement action at the relation's center.
In comparison, the \textit{schema:AddAction} is defined as, ``the act of editing by adding an object to a collection" \cite{SchemaAdd}.
The action only involves the object and the new state of the collection, not involving any of the collection's prior lineage.
Schema.org defines the \textit{schema:DeleteAction} as, "the act of editing a recipient by removing one of its objects," \cite{SchemaRem}.
The concept aligns well with other versioning systems, although deletion may be a strong assertion.

\section{Documenting Versions} \label{sec:changelog}

Change logs, artifacts resulting from the versioning process, play a major role filling in gaps between versions.
The logs document changes and explain, in human language, motivations behind the modifications \cite{uel1037}.
Since identifiers denote that a change has occurred, the logs provide details on how the changes modify an object's attributes.
They demonstrate a need and utility in understanding the deeper content of change beyond knowing that an object did transform.
While some data sets will contain a change log, software projects have normalized their use in version release documentation.
As a result, these projects provide a basis for understanding the value of change logs to data sets with multiple versions.
The change log's common drawback is the limitation to only human readable text.
Wider adoption among data sets may be possible by making these texts machine computable.

Open source projects use change logs more consistently than data projects, which usually sport only use documentation \cite{German03automatingthe}.
Logs play an important communication role in these projects since developers can contribute without having been part of the original development team.
The change logs allow developers to link bugs and errors with their corrections in new versions of the code \cite{Chen:2004:OCL:990374.990391}.
The links gives insight into motivations behind particular design decisions.
Logs linked with version releases also provide feedback to the user community that corrections have been addressed, in addition to ensuring that improvements drive modifications to the code base.
An identifier cannot communicate these qualities while remaining succinct.
Some research has been done to determine the health of a development project based on the number and length of change logs released over time \cite{German03automatingthe}.
Little work has been done to make change logs machine-computable, as many of these documents remain in human-readable text only.
Research done involving change log content must manually link entries with computable meta-data such as the introduction of new features with the emergence of new bugs \cite{6132954}.
While machines may still be significantly removed from the ability to comprehend the impact of changes made to a data set or software code, they are currently opaquely blocked from consuming any of the content within logs more than understanding they contain text.
The transition between different versions of large data sets is then left largely up to the human user's ability to understand and process the modifications mentioned within the change log.

\section{The Versioning Use Case} \label{sec:usecase}

The research questions addressed by work in the following chapters revolve around different aspects of the flow in use case shown in Table \ref{table:UseCase}.
\begin{table}
	\caption{Versioning Use Case Table}
	\label{table:UseCase}
	\centering
	\begin{tabular}{|L{14cm}|}
		\hline
		\textbf{Use Case Name}: New Version Publication \& Retrieval\\
		\hline
		\textbf{Goal}: Record a new version of a data set and provide it to a data consumer.\\
		\hline
		\textbf{Summary}: The Producer creates a new version of their data and must record it to the Versioning System while providing the Consumer with the data\\
		\hline
		\textbf{Actors}: Producer, Consumer\\
		\hline
		\textbf{Preconditions}: Producer has supplied some data to the Versioning System.   Consumer has retrieved the data from the Versioning System.\\
		\hline
		\textbf{Triggers}: Producer provides a different version of the data to the Versioning System.\\
		\hline
		\textbf{Basic Flow}: 
		\begin{enumerate}
			\item Producer places the next version into the Versioning System.
			\item Producer may repeat the previous action 0 or more times.
			\item Consumer checks the data to see if there are changes.
			\item If there are pertinent changes:
			\begin{enumerate}
				\item Consumer retrieves the updated data.
			\end{enumerate}
		\end{enumerate}\\
		\hline
		\textbf{Alternate Flow}:
		\begin{enumerate}
			\item Producer places the next version into the Versioning System.
			\item Producer notifies Consumer that an alternate version is available.
			\item Consumer retrieves the updated data.
		\end{enumerate}\\
		\hline
		\textbf{Post Conditions}: Producer has made the alternate version available.  Consumer possesses the pertinent newly available version.\\
		\hline
	\end{tabular}
\end{table}
The use case begins when a data producer wishes to distribute the next version of a data set.
While versions for observations of phenomena is often sequential, alternate versions can deal with older data or occur concurrently with the object the alternate changes so the adjective new or newer is avoided when possible.
The use case features the data producer and data consumer as actors using the versioning system as an intermediary.
The actors do not follow separate use cases because each actor relies on the actions of the other actor to contextualize the actions in the use case.

The basic flow of the use case is seen in Figure \ref{UCD1} where the producer logs a version into the versioning system, and the consumer retrieves that data version.
\begin{figure}
	\centering
	\includegraphics[scale=1]{figures/UC_Diagram1.png}
	\caption{Basic Flow Use Case Diagram.}
	\label{UCD1}
\end{figure}
At a later point in time, the producer can add any number of corrections to the versioning system.
The repeated corrections reflect a practice of passively logging additional changes.
At some final time after 0 or more corrections, the Consumer returns to check the versioning system for any changes to the data set the Consumer retrieved.
If there are changes to parts pertinent to the Consumer, the actor retrieves the corrected data.
In the interaction, the Producer only passively provides additional versions of the data while the responsibility of remaining up-to-date lies with the Consumer.
The ability of the  versioning system to communicate to the Consumer that the data has changed determines whether the use case succeeds or fails for the data consumer.
The relationship creates a Producer/Consumer dynamic which influences the performance of the versioning system.

The alternate flow in Figure \ref{UCD2} requires additional information from the Consumer which is a means of notification.
\begin{figure}
	\centering
	\includegraphics[scale=1]{figures/UC_Diagram2.png}
	\caption{Alternate Flow Use Case Diagram.}
	\label{UCD2}
\end{figure}
The flow begins the same as the basic flow, but after a single correction, the Producer notifies the Consumer that the data has changed.
The notification causes the Consumer to come and retrieve the updated data set.
The alternate flow poses a few problems in that the Producer must now manage notification information, but the Producer must also notify all consumers of the data which may cause scalability issues.

\subsection{Research Question 1: What has changed?}

The primary research question addressed in the following chapters pertains to step 3 in the basic flow of the use case.
In order to execute this step, the Consumer must have a means of determining whether the data set is different from the one the Consumer currently possesses.
In a large number of software projects, especially open-source projects, change logs document for the producer changes to the code base and communicate alterations to the Consumer.
Very few data sets provide detailed change logs along with versions in the versioning system.
Either general descriptions of changes are made or new usage documentation is provided, requiring the data consumer to manually acclimate to changes.

The prevailing hypothesis in addressing Research Question 1 is that automating the generation of change logs will provide detailed change documentation for even large data sets.
Different standards exist across various national agencies, but few fundamentals have been established on what needs to be included and how the information is to be captured or presented.
A linked data versioning model would provide a standardized basis for discussing version change as well as allow tools to be developed to assist in step 3 for large data sets of which Consumers may only use a part.

\subsection{Research Question 2: How much has changed?}

A follow-up query to Research Question 1 is that once the changes have been determined, Consumers need to evaluate the impact of significance of the changes.
At the moment, Producers regularly make dot-decimal identifiers pull double duty in organizing versions sequentially and implying change magnitude.
Version counts, as shown in Chapter \ref{ch:graph}, provide a very bland description of the changes in a version, lacking nuance and effected by multiple outside practices.
Using the versioning model to answer Research Question 1, the changes can be broken down into \textbf{Add}, \textbf{Invalidate}, and \textbf{Modify} (AIM) classifications which provide a more encapsulating description of a version transition.
The AIM change counts would tie change metrics back to data differences rather than to versions.
The AIM changes also provide a means to compare the difference between the amount of change declared by the Producer and quantity of change as seen by various Consumers.

\subsection{Research Question 3: How fast does the data change?}

An important idea to understand is how often will the consumer need to execute the versioning use case.
Research takes time and if the data is unstable, new versions of the data may be produced before results can be extracted from the data, invalidating the findings.
With the ability to measure changes from Research Question 2, changes in each version can be tied back to time.
Versions package and release changes in a bundle, disconnecting the changes from time but limiting the data volatility to an acceptable level.
The hypothesis is that versions are masking the actual change rate, the giving a false sense to the expected change in the data over time.

\section{Hypothesis Statement}

The work in this dissertation seeks to prove three hypotheses.
In the first hypothesis, provenance concepts for revisions need to be diversified in order to properly capture change information.
Prior versioning models indicate that versioning graphs will need to utilize a tiered approach to capture the relation between objects and their more granular attributes.
These tiers unify the practices already established by provenance models with the semantics defined by versioning models.
The content and format of change logs give insight into the information desired by version users.
The versioning graphs in this dissertation will explicitly deal with data sets since software versioning is a more developed field.
An additional assumption in constructing the versioning graph is that the objects used in the comparison have already been established as versions of each other.
That is, objects in the versioning graph belong to the same \textbf{work} and share the same tier.

The second hypothesis is that versioning graphs can calculate a more accurate change distance by counting the number of changes.
Current methods rely on vague data producer evaluation, communicated through changing version identifiers.
Because version graphs will catalog individual changes, a count based on the different types of changes are expected to produce a more revealing change metric.
The distance can be verified by comparison with the amount of change indicated by the version identifier.
The hypothesis relies on an assumption that all changes of the same type within a work have the same or similar significance.
The evaluation stems from the observation that change logs exist to capture the differences between sequential versions.

The third hypothesis is that versioning graphs will enable the automated creation of machine readable change logs.
Assuming that natural language processing cannot understand and quantify the change in a log, quantifying the log's information remains difficult.
The results will show that a comprehensive log can be generated which is both human and machine consumable.
The approach will deal specifically with spreadsheet data sets since they provide a flatter, more consistent context to capture.

\section{Contributions}

%Add chapter number and references.

In Chapter \ref{ch:graph}, I develop my first contribution, the idea of a versioning graph, through the process of addressing Use Case 1.
Versioning graphs capture differences between objects, not the course used to create those objects, differentiating themselves from provenance graphs.
The versioning graph enables my second contribution, a process to automate machine-readable change log creation covered in Chapter \ref{ch:changelog}.
The contribution eases consuming very lengthy logs, which data sets often produce, as well as enabling searchability and discoverability of changes affecting the version.
My third contribution, discussed in Chapter \ref{ch:distance} is using a versioning graph to provide a quantitative basis for determining change distance.
The resulting distance measure comes in three parts to both characterize the distance and provide a basis for version identifier assignment.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
