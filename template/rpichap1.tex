%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                                                 %
%                            CHAPTER ONE                          %
%                                                                 %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{INTRODUCTION}

%%%%%%  Data changes and grows, we have to track it --> Data Quality?
John C. Maxwell once said, "Change is inevitable.  Growth is optional."
While this inspirational quote refers to the human character, it also holds true for scientific datasets.
With changing technology, data collected by researchers grew at an astounding rate.
NASA's Atmospheric Science Data Center reported a growth from hosting around five million files to twenty million files between 2001 and 2004 \cite{Barkstrom_digitallibrary}.
The ATLAS project at CERN reports that it generates on the order of four thousand new datasets per day from experimental tests alone\cite{Branco2008}.
This explosion of data marks a change in the perspective of research from a software driven approach, where programs are used to confirm results using data, to a data driven approach, where vast quantities of data is integrated to demonstrate trends and produce results.
Instead of simply using computers to assist in performing a complex computation, researchers now use broad data collections to inform advances in science.
This means that data quality needs to be ensured in order to extract valid conclusions from large data sets as humans can no longer manually curate the data mining process.
However, the need for quality assurance means that a portion of the files generated do not contribute to data set growth since data has not been added, simply modified.
Many NASA datasets have required re-processing of their data, either to improve data quality or to correct for errors \cite{barkstrom2014earth}.
Data traceability now becomes particularly important to identify sources that contribute to improved data quality.
It creates a need to understand not only that a data set has changed, but to also understand how much a data set has changed.
Data versioning is the method of tracking the changes performed on a data set and determining the extent to which it has changed.
In this document, data versioning is approached using technology provided by semantic technologies and applying them to artifacts currently generated by scientific data sets.

\section{Data Set Proliferation}

\subsection{Explain why we choose the data types that we did}
Data has existed long before computers, populating the storage space of filing cabinets and data closets the first transistor radio.
It then comes as no surprise that libraries and library sciences provide the early methods of data management.
The challenges and goals that face physical libraries remain valid even as data collection migrates to electronic alternatives.
Digital storage and the Internet has opened new opportunities and methods to administer book data by separating logical representations and physical representations \cite{Barkstrom_digitallibrary}.
However, the migration has not been without its problems.
Early citations used stagnant Uniform Resource Locators (URL) to refer to online documents, but this would lead to a condition known as link rot where moving the document would invalidate the URL \cite{Lyons2005}.
This eventually led to the development of Persistent URLs (PURL) which also succumbed to link rot, and this eventually led to the distributed Digital Object Identifier (DOI) system used to track documents today.
The DOI network provides a robust system to track documents, but when tracking data, it faces difficulty following the rate of change with some more volatile data sets.
Distribution organizations assign a DOI whenever a new edition of a document becomes available, and due to the publication process, documents change very rarely so a new DOIs are rarely necessary.
However, data sets are products and thus succumb to the iterative process of error correction and growth.
Data collection often continues on after initial publication.
DOI distributors treat new files like new sections to a paper and changes to files as edits so a new identifier must be issued to the data set.

For similar reasons, treating data as documents produces problems when applying technologies from software management \cite{tichy1985rcs}\cite{Chien:2000:VMX:646544.696357}.
Structure provides the most significant distinguisher between data and software since a data set with a removed file remains usable but a software project would break.
The function of code comes from its content, but the function of data comes from its ability to store and organize data.
This should not be confused with data formats which impose structure onto data in much the same way programming languages provides a medium to express actions.
However, exporting data in different formats is currently easier than exporting code into different languages.
Data sets do not represent a single object, unlike a software project\cite{Chacon:2009:PG:1618548}.
Data sets are compact representations of all possible subsets of the data set, which are also datasets.
For this reason, the structures of data sets and software becomes incompatible and software versioning technologies are insufficient to capture this nuance.

The techniques employed by these technologies, however, can remain applicable to data sets and are often necessary when communicating change data to users.
Version producers often refer to versions using numbers in the dot-decimal style.
While the values often signify the Major-minor numbers associated with the version, the names remain meaningless and can arbitrary assignment such as Ubuntu released numbered by Year-month values \cite{Ubuntu}.
The arbitrary nature of the numbers often entails referring to versions by English nicknames instead.
Such a regular method of naming release versions also means that determining the magnitude of change between two releases becomes impossible.
Numbering the version this way, however, does allow computers and readers to quickly parse the version name and discern that a change has occurred, but little value exists beyond that \cite{Dijkstra1994}.
The technique of distributed and federated employed by GIT does provide significant value to modern methods of versioning data \cite{cederqvist2002version}.
As data workflows and data set dependencies grow, their volatility also expands, meaning that they become more likely to generate new versions.
The federated approach available in the GIT environment allows developers to establish change dams that collect modifications and releasing the data at regular intervals,reducing the changes to a manageable flow.

\subsection{Digital Libraries}
Data as documents vs Data of documents.

Data spreadsheets resemble books because they have a clear hierarchical structure.
Each spreadsheet is a full unit by itself.  The Excel files themselves are known as workbooks.
XML documents also have a clear hierarchy.
Due to the rapid growth of new technologies, the need for proper data versioning extends beyond major agencies and even applies to smaller projects \cite{burrows2006review} \cite{Tagger2005} \cite{Stuckenholz:2005:CEV:1039174.1039197}.
Indeed, version control systems can provide contexts for modifications when combined with error detection systems, providing a more complete picture of the system's behavior \cite{Fischer2003}.

Early work in the field of digital data versioning begins in the library sciences.
As the computing field grows, many data publications are now being provided in digital formats as well as print.
The challenges of tracking a digital library are often the same as those faced in data repository management \cite{Wiil:2000:RDH:338407.338517}.
Not only do librarians often have to simultaneously work with several versions of the same material, but they also must pay attention to the structure of their collection as well as its content.
They have also been used to track and cite datasets, but unlike documents, datasets have a tendancy to change after publication.
As a result, new versions of data would get a new DOI to prevent confusion, and this unnecessarily consumes the available handles and the time of institutions assigning handles \cite{Lyons2005}.
Other identifier systems have also had problems with maintaining healthy links to data as it changes and moves, but systems using newer technology have been able to achieve success in localized networks.
Built on XML and web service technology, the Mellon Fedora project linked together various disparate digital library collections at the University of Virginia  \cite{Payette2002}.
Digital publications not only include books and journals, but also grew to include web pages and wikis, requiring new scalable methods to track changes on these publications \cite{Berberich:2007:TMT:1277741.1277831}.

\section{Unifying multiple systems}
\subsection{Grid}
In the early 2000s, versionings systems were being prepared for the future of grid technologies.
The grid provided a unique environment that had to handle a variety of inputs, and therefore, different input data could run on distinct sets of grid services.
This meant that different versions of the same data could be generated by differing services on the same grid \cite{Kovse2003VGridAVS}.
A versioning environment would also have to be general enough to accommodate disparate input types.
CERN grid for the Compact Muon Solenoid experiment separates the physical and logical storage of files, allowing multiple users to refer to the same file without needing to copy the file across the grid \cite{Holtman:687353}.
Versioning policy could not exist in this case if a clear replication policy did not exist since changes to the underlying file can have repercussions with its replications across the grid.
The versioning method relies on predictable replication in order to reliably communicate change to grid users.

\subsection{Heterogeneous systems}
While innovative, the grid was neglected in favor of networked heterogeneous systems.
The proliferation of mobile computational devices such as laptops and smart phones created a supply of small data repositories that were too compact to warrant the use of grid technologies.
Baker and Yarmey describe a method of networking together small, volatile datasets with larger, versioned data distribution centers for environmental data \cite{Baker2009}.
Heterogeneous systems encounter challenges with data integration resulting from non-uniform data interfaces.
Autonomous solutions to understanding change grows in importance as evidence grows that speedy and relevant changes play a significant role in successful system function \cite{Bouzeghoub:2004:FAD:1012453.1012464}.
The key often seems to be using XML to provide an common language propagating change across heterogenous systems.
The need to propagate changes becomes more apparent as autonomous units grow and share data \cite{Systems02champagne:data}.
Semantic technology's growth now provides a new method of propagating data change that can not only be universally consumed in formats like XML but also encodes meaning to each of these changes.
\subsection{Transition to data driven systems}

\subsection{How much does data grow?}

\section{Data Quality/Provenance}

Provenance describes the sequence of actions and the individuals involved in generating a particular artifact.
In the case of art, provenance would include a history of previous owners to ensure the validity of the work, in short, to ensure its quality.

In the early days of computing, hardware constraints restricted the size of data to very manageable quantities.
However, recent advances in storage methods has produced devices, even on a personal level, with the capability to store data that far exceeds the size of the programs that utilize them.
It is then no surprise that the development of versioning management tools for software 
However, the proliferation of powerful and mobile computing devices has allowed researchers to generate data faster than it can be tracked \cite{Bose:2005:LRS:1057977.1057978}.
Data processing is automated so data quality needs to be ensured since a human will not be able to determine data quality for each data entry individually.

The information that details the activities and agents involved in generating a data entity is known as provenance.
Studying the structure of a data's provenance has provided insight into behavior and quality by revealing the qualities of the contributing data sources and code sources \cite{dai2014provenance} \cite{Cavanaugh2002}.
Provenance management differs from organization to organization, but most generally agree the core factors of scripts, calibrations, and inputs form the core components of lineage tracking \cite{Barkstrom2003} \cite{Branco2008}.
Software revision management tools such as Git and SVN also keep track of provenance information when logging new commits to a project, using branches and merges  \cite{Chacon:2009:PG:1618548}.
Provenance is a powerful mechanism to follow data quality changes or error introduction .
The information also gives data warehouses a mechanism to evolve and adjust their ability to provide better data \cite{Vassiliadis1999}.
It is, however, important to distinguish between the contributions of provenance to change tracking and the contributions of versioning.
Provenance allows for the identification of change localities; it signals where changes in data have occurred.
However, provenance does not elaborate on the extent of the alterations made, not should it.
The priority of provenance tracking is understanding where and how a data object was generated, not it's relationship to other data products \cite{Bose:2005:LRS:1057977.1057978}.
In the field of semantic technologies, the W3C recommendation, PROV provides a data model to encode provenance information into searchable graphs.
It has played a significant contribution in maintaining the quality and reproducibility of datasets and reporting in the National Climate Assessment \cite{Tilmes2012,Ma2014191,Ma2014}.
PROV expresses relationships between versions with the wasRevisionOf or alternateOf property.
The properties do not allow for more elaboration as to what changes were made to transform from one version into its alternate.
Versioning provides the context for a change by providing the comparison between data objects.

\subsection{Changelogs}

Changelogs, sometimes called patch notes, are artifacts resulting from the versioning process often found in major software projects.
They detail the changes made within the system and some will have explanations on the motivations behind changes \cite{uel1037}.
These logs provide a great source of value to developers as they can be used to give insight to the health of a software project \cite{German03automatingthe} \cite{6132954}.
Changelogs also allow developers to link bugs and errors with their corrections in later versions \cite{Chen:2004:OCL:990374.990391}.
Despite these contributions, changelogs are rarely found along with data.
Possible reasons for this omission may include the size of data  as it may become difficult to scalably communicate data change (Data exceeds ability to track reference) or that the ease of data acquisition and storage inclines towards simply using the latest version.
As a result, new dataset versions are often accompanied by usage documents, but documentation on version transitions are omitted.
This creates a impediment to making data forward and backwards compatible, costing data consumers both time and money.

Changelogs are also difficult to consume as they are often built to be read only by humans.
The transition between different versions of large datasets is then left largely up to the human user's ability to understand and process the modifications mentioned within the change log.
In the absence of any sort of change notes, it is up to the consumer to be able to locate the points of differences between the versions they are working among.
Therefore, providing an machine consumable changelog would accelerate and assist in navigating through dataset changes and error corrections.

\section{Provenance Distance}

Data workflows have become complex and sized in such a way that even subtle changes may have meaningful effects \cite{TILMES2011548}.
One approach is following the provenance of the new data and identifying potential differences between how the data has become generated.
Since provenance graphs have both directed edges and labeled nodes, there are a few methods of measuring similarity available beyond basic isomorphism \cite{Cao2013} \cite{Gao2010} \cite{Goddard:1996:DGU:246962.246972} .
This similarity measure is known as the provenance distance between the two data objects.
A concern that arises with this approach is that, as previously stated, provenance only provides a context for versioning.
Consider an example where there are versions 1 and 2 of a data object, both compiled from a reference source.
Since there are two versions, it is understood that the versions are different so the comparison extends to their reference sources.
If both versions were compiled from the same source, it can only be concluded that the compilation methods were different.
There also exist methods that compare workflows based on quality criteria that leverages provenance to bound quality of service \cite{2015:CAA:2778374.2778504}.
In the case of the NASA datasets, those methods should not have changed and no further insight has been found.
If the reference sources have been changed, then it is expected that there would be a new version or data object and no new information has been found.
This reasoning motivates the development of a versioning method that utilizes changelogs and RDFa to determine versioning distance.

\section{Data Versioning Operations}

Architecture has a principle that says form follows function, but, for data, form equals function.
As a result, data has as many different forms as it has functions.
Biological experiments often use data within cyclical data workflows where outputs are immediately fed back into new experiments \cite{Tagger2005}.
Even though the goal of the experiment is the final data set, all the intermediary data sets provide significant value in reaching the goal.
Libraries store data about their collections in large databases where both old and new versions of literature need to be maintained \cite{Wiil:2000:RDH:338407.338517}.
Some data exist in such a highly constrained environment that it must be managed at near the hardware level \cite{Flouris04clotho:transparent}.
The challenge no longer becomes generating data, but instead, fitting the data into a format that users find useful and can consume.

The challenge of data versioning systems is to provide a unifying environment that can handle the plethora of forms and functions of its data.
At its core, versioning systems only need to concern themselves with three operations: addition, deletion, and modification.
Most literature surveys do not realize the significance of this commonality as this means that versioning methods can be described by delineating how each operation is approached by a system \cite{Tagger2005} \cite{burrows2006review}.
Data addition generally constitutes the least complicated versioning operation because it interacts the least with pre-existing data.
However, new data does share context with pre-existing data and provides a method of measuring data set growth.
Since data sets no longer have to be used in its entirety and can be freely subsetted, a data set's complexity increases significantly with its growth.
Every new file added to a data set doubles the number of available subsets.

Data deletion, however, has a more philosophical difference between systems.
From the perspective of a versioning specialist, data should never be deleted since knowing why data was excluded is as important as knowing why data was included.
The software versioning manager GIT uses a method of compressing older data to conserve space without deleting the data \cite{Chacon:2009:PG:1618548}.
Pragmatically, this is not always possible due, generally, to the physical constraints of storage space.
In high energy physics, observational data often cannot be re-collected due to cost, and as a result, poor quality data cannot be re-processed or replaced \cite{Cavanaugh2002}.
The decision in this document is to use the term invalidation when referring to data removal operations as it implies that whether permanently deleted or not, there exists a more valid alternative.

Data modification encompasses the most involved data versioning operation.
As a result, it often comprises a majority of the description of a data versioning service.
In truth, data modification can be summarized as the invalidation of an instance of a data object - which can be a file, a record, or anything in a data set - followed by the addition of a new instance of that data object.
However, this kind of operation is used so often to fix errors and update data sets that it is considered a unique operation.
Modification owes its complexity to interacting with both pre-existing data from the invalidation stage and new data from the addition stage.
However, this compound relationship fully contextualizes the relationship the operation has in relating the old data and the new data.
In some cases, this only provides forward or backwards references between data versions, but having both gives users context for data's current state and update to new data \cite{Klein01ontologyversioning}.

Due to the ubiquity of the data addition, invalidation, and modification operations in versioning systems, the conceptual versioning model presented in Chapter 3 centers around capturing the relationships established by each of the operations.
While other functions exist commonly in versioning systems such as object locking to prevent simultaneous conflicting changes, viewing to see the version an object belongs to, and branching to allow distributed modifications, these functions comprise the space of utility operations that support the three core processes.

\subsection{Types of Change}

Focus on change types because focusing on the values has comparably less value.
When working with large data sets, individual changes are less important than changes made to the set abstractly.

Change is the fundamental characteristic in the study of versioning.
Continuing the comparison with software, the measure of change in applications is primarily functional.
Data, on the other hand, has not only function but also structure.
While swapping the ordering of columns in a spreadsheet may not effect the usability of the data, it could cause issues with codes that use the data.
This highlights the idea that many different forms of change exist with differing degrees of intensity.
In this document, change is viewed as having three general categories: scientific, technical, and lexical.
The most impactful change, scientific, denotes changes that have modified the fundamental science used to generate the dataset.
Changes in algorithms or sampling methods generally fall under scientific modifications as they effect the base assumptions and possible error within the dataset.
Technical impacts do not change the underlying science of the data, but impose a large enough change as to warrant notice.
Structure alteration and unit conversions count as technical changes since the dataset now needs to be consumed differently but remains valid for use.
Lexical changes belie the transformations that can best be described as corrections.
Filling in previously missing values or fix erroneous values may be lexical changes.

The exact category that a particular change falls into can be controversial.
The decision to change concentration units from parts per million to milligrams per milliliter poses a Technical change for a data producer.
However, for a data consumer, the change may be viewed as a Scientific change as it invalidates the methods they had previously used.
This conflict in view illustrates the data consumer-producer dynamic.
In general, data producers are in control of the methods of versioning, but data consumers determine the classification of a data change.
Reference 19 demonstrates the viewpoint as producers tend to use versioning systems to ensure data quality of service through audits and recovery tools.
Meanwhile, a consumer will analyze the historical changes and determine the impact this may have to their data use.
As a result, this means that data versioning systems must communicate a dynamic view of the changes in a system contextualized by the user of that data.

\section{Conclusion - Thesis Statement}

\section{RDFa}

The modifications to implement a machine readable changelog should still remain in a form viewable by a human user.
The subjectivity of versioning information means that domain scientists should still be able to interpret changes as they relate to their application.
Resource Description Framework in Attributes (RDFa) allows web developers encode information XHTML and HTML documents using standardized ontologies.
The developer would use the attributes in RDFa to augment the existing HTML tags to provide extra meaning that a web crawler can extract.
The displayed text would remain unchanged signifying that if a changelog could be produced in HTML, then data producers can embed machine readable versioning information into a changelog using RDFa while still leaving it human readable as well.


%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
