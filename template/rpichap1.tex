%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                                                 %
%                            CHAPTER ONE                          %
%                                                                 %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{INTRODUCTION}

%%%%%%  Data changes and grows, we have to track it --> Data Quality?
John C. Maxwell once said, "Change is inevitable.  Growth is optional."
While this inspirational quote refers to the human character, it also holds true for scientific datasets.
With changing technology, data collected by researchers grew at an astounding rate.
NASA's Atmospheric Science Data Center reported a growth from hosting around five million files to twenty million files between 2001 and 2004 \cite{Barkstrom_digitallibrary}.
The ATLAS project at CERN reports that it generates on the order of four thousand new datasets per day from experimental tests alone\cite{Branco2008}.
This explosion of data marks a change in the perspective of research from a software driven approach, where programs are used to confirm results using data, to a data driven approach, where vast quantities of data is integrated to demonstrate trends and produce results.
Instead of simply using computers to assist in performing a complex computation, researchers now use broad data collections to inform advances in science.
This means that data quality needs to be ensured in order to extract valid conclusions from large data sets as humans can no longer manually curate the data mining process.
However, the need for quality assurance means that a portion of the files generated do not contribute to data set growth since data has not been added, simply modified.
Many NASA datasets have required re-processing of their data, either to improve data quality or to correct for errors \cite{barkstrom2014earth}.
Data traceability now becomes particularly important to identify sources that contribute to improved data quality.
It creates a need to understand not only that a data set has changed, but to also understand how much a data set has changed.
Data versioning is the method of tracking the changes performed on a data set and determining the extent to which it has changed.
In this document, data versioning is approached using technology provided by semantic technologies and applying them to artifacts currently generated by scientific data sets.

\section{Data Set Proliferation}

Data has existed long before computers, populating the storage space of filing cabinets and data closets, before the first transistor radio.
It then comes as no surprise that libraries and library sciences provide the early methods of data management.
The challenges and goals that face physical libraries remain valid even as data collection migrates to electronic alternatives \cite{rohtua}.
Digital storage and the Internet has opened new opportunities and methods to administer book data by separating logical representations and physical representations \cite{Barkstrom_digitallibrary}.
It has also added a plethora of new content types such as wikis, blogs, and other document formats which have never seen physical print.
All these new documents need a form of data management \cite{Berberich:2007:TMT:1277741.1277831}.
However, the migration has not been without its problems.
Early citations used stagnant Uniform Resource Locators (URL) to refer to online documents, but this would lead to a condition known as link rot where moving the document would invalidate the URL \cite{Lyons2005}.
This eventually led to the development of Persistent URLs (PURL) which also succumbed to link rot, and this eventually led to the distributed Digital Object Identifier (DOI) system used to track documents today \cite{Duerr2011}.
The DOI network provides a robust system to track documents, but when tracking data, it faces difficulty following the rate of change with some more volatile data sets.
Distribution organizations assign a DOI whenever a new edition of a document becomes available, and due to the publication process, documents change very rarely so a new DOIs are rarely necessary.
However, data sets are products and thus succumb to the iterative process of error correction and growth.
Data collection often continues on after initial publication.
DOI distributors treat new files like new sections to a paper and changes to files as edits so a new identifier must be issued to the data set.
This behavior becomes entirely too slow as data providers begin to allow users to dynamically generate data products from existing data according to their needs \cite{Barkstrom2003a}.

For similar reasons, treating data as documents produces problems when applying technologies from software management \cite{tichy1985rcs}\cite{Chien:2000:VMX:646544.696357}.
Structure provides the most significant distinguisher between data and software since a data set with a removed file remains usable but a software project would break.
The function of code comes from its content, but the function of data comes from its ability to store and organize data.
This should not be confused with data formats which impose structure onto data in much the same way programming languages provides a medium to express actions.
However, exporting data in different formats is currently easier than exporting code into different languages.
Data sets do not represent a single object, unlike a software project\cite{Chacon:2009:PG:1618548}.
They are compact representations of all possible subsets of the data set, which are also datasets.
This can be demonstrated simply in considering a data set where a user orders a file Y and over the course of a year all files but Y are modified and replaced.
The user only wants to know when their file Y changes but remains unconcerned when other files change within the data set.
This is not so with software projects.
For this reason, the structures of data sets and software becomes incompatible and software versioning technologies are insufficient to capture this nuance.

The techniques employed by these technologies, however, can remain applicable to data sets and are often necessary when communicating change data to users.
Version producers often refer to versions using numbers in the dot-decimal style \cite{Stuckenholz:2005:CEV:1039174.1039197}.
While the values often signify the Major-minor numbers associated with the version, the names remain meaningless and can arbitrary assignment such as Ubuntu released numbered by Year-month values \cite{Ubuntu}.
The arbitrary nature of the numbers often entails referring to versions by English nicknames instead.
Such a regular method of naming release versions also means that determining the magnitude of change between two releases becomes impossible.
Numbering the version this way, however, does allow computers and readers to quickly parse the version name and discern that a change has occurred, but little value exists beyond that \cite{Dijkstra1994}.
The technique of distributed and federated employed by GIT does provide significant value to modern methods of versioning data \cite{cederqvist2002version}.
As data workflows and data set dependencies grow, their volatility also expands, meaning that they become more likely to generate new versions.
The federated approach available in the GIT environment allows developers to establish change dams that collect modifications and releasing the data at regular intervals,reducing the changes to a manageable flow.

\subsection{Unified Systems}

Working with data as documents leads to the shortfall of technologies, but working with the data of documents has led to significantly greater contributions.
Many libraries often work in collaboration in order to provide a wide selection of texts over a limited number of physically available documents.
The University of Virginia demonstrates the ability to achieve a unified library system using a combination of XML and web service technologies of their disparate assemblage of libraries \cite{Payette2002}.
The challenge involves providing a common landscape in order to compare the quality requirements imposed on the repositories.
Versioning systems provide a notable mechanism to make this decision as quality determines when to generate new versions and what items belong to the same groupings of data.
The comprehensiveness of XML and web technologies also allows this approach to apply to other systems and research areas as well.
This becomes particularly relevant as innovations in computation technology generates small, volatile data sets to integrate into larger data managers \cite{Baker2009}.
In this application, the data food chain then becomes represented by smaller applications generated in situ and then unified with other data sets as they move up the food chain to a large, unified data distribution center.

Unified libraries represent a part of a larger collection of systems that rely on the propagation of data through heterogeneous systems to produce rapid complex solutions.
The grid provided a unique environment that had to handle a variety of inputs, and therefore, different input data could run on distinct sets of grid services.
This meant that different versions of the same data could be generated by differing services on the same grid \cite{Kovse2003VGridAVS}.
The CERN grid for the Compact Muon Solenoid experiment separates the physical and logical storage of files, allowing multiple users to refer to the same file without needing to copy the file across the grid \cite{Holtman:687353}.
While the structure and construction of the grid reduced the uncertainty introduced by varying hardware, it raised questions on data quality by abstracting the transparency to underlying services.
Cloud services have recently replaced the grid due to its flexibility in the services available to its autonomous systems.
As the scale and complexity of autonomous systems grow, it becomes more difficult for one system or organization to manage all the circles necessary to produce data deliverables.
The ability to propagate relevant data change data across autonomous systems then assures valid quality in interactions between domains \cite{Systems02champagne:data}.
Not only does this ensure uniformity through system interaction, but it also ensures transparency with respect to the data and methods used to produce conclusions \cite{Tagger2005}.
This often means that systems will need to negotiate a contract and establish a mutual interface to exchange data.
Occasionally, this contract can be formal, but more ideally, the establishment of a standard lineage model or format would allow a greater variety of systems to interact with each other without needing lengthy contractual exchanges.

\section{Data Quality/Provenance}

The fundamental challenge to determining data quality, its subjectivity, needs clarification.
Conceptually, a data set on desert climate likely has very poor data quality and relevancy to a study in whale biology.
However, in a more quantitative sense, that same desert climate data can have excellent quality with respect to its correctness, expression, and traceability.
With the hybridization of data sets from disparate agencies to provide big data solutions, collaborations plays an ever present role in achieving broad, valid findings.
This requires good quality data and the ability to determine when data with better quality becomes available. \cite{Wiil:2000:RDH:338407.338517}.
The primary focus, generally, involves tracing the lineage of artifacts and activities that lead to the current data.
This provides insight into possible sources of error as well as validating the assumptions made in generating a data set for future use.

There are several characteristics that can describe a data's quality, but the one most relevant to data versioning is provenance.
It describes the sequence of events that lead to the construction of an object  \cite{dai2014provenance}.
In art this describes the sequence of people who have had ownership of a piece of artwork.
For data, provenance relates the history inputs and operations that result in a data object such as a plot or data set.
NASA defines three levels of data processing, for example, that encompass the stages required to turn a raw signal from satellite instruments into physical measurements into global aggregate summaries \cite{Barkstrom2003}.
Each stage computes a dataset using a collection of input data, processing scripts, and calibration values.
This collection forms the provenance for the resulting level of data, but because lower level data become inputs for higher level data, its provenance assimilates into the lineage of all resultant data sets.
For example, Level 1 data's provenance is described by Level 0 data and the scripts and the calibration inputs that were required to turn Level 0 data into Level 1.
Because Level 1 data is then used to generate Level 2 data, which in turn is used to generate a Level 3 data set, the Level 1 provenance is also included in Level 3 data's provenance.
It is part of the sequence of events that led to the generation of the Level 3 data set.

As data sets grow, this process becomes even more confusing to coordinate so version control systems often manage provenance.
Current research endeavors to provide high quality data clearly becomes more formalized as data becomes concentrated in massive data warehouses \cite{Vassiliadis1999}.
The focus of a majority of versioning research focuses on lineage retrieval which becomes ever important as evidence grows that researchers generate data faster than they can reasonably track \cite{Bose:2005:LRS:1057977.1057978}.
This poses a particularly difficult problem as provenance provides a potent means of data auditing.
With provenance, data producers ensure the trustability of their data inputs, either ingested from external sources or integrated from internal data sets.
Fairly reassuring results have been found when combining lineage management and error reporting systems \cite{Fischer2003}.
The errors provide a context for the changes made to advance the lineage of the data set and the version manager demonstrates that a problem has been addressed and how it was corrected.
This system becomes extremely important when considering that agency funding often depends on the ability to account for the value of a project's dataset \cite{Cavanaugh2002}.
The data analytics required to determine the value of data collected by a project also requires the provenance to ensure that the analysis is also reproducible.
The basic provenance often collected by hand now needs to be collected automatically in order to facilitate collaboration, especially with projects that are farther away from the data.

Early attempts at encoding provenance data into semantic models include the development of the Proof Markup Language \cite{daSilva2006381}.
While this was originally developed to express inference reasoning through traceable graph relations, the model can also be used to express the provenance of products using the same transitions.
The power is that it is able to use terms defined on the Semantic Web to construct inferences.
This early demonstration of the ability for web based semantic technologies also expresses complex relations in a way that can be reasoned over and computed.
It then allows for autonomous solutions to understanding change as data freshness begins playing a significant role in successful system function \cite{Bouzeghoub:2004:FAD:1012453.1012464}.

Not long after began the development of the Open Provenance Model (OPM) \cite{moreau2008open}.
Driven by the uncertain needs and sometimes conflicting conventions of different scientific domains, the model sought to find a method to standardized the way in which provenance data is captured while also keeping the specification open to accommodate current data sets through the change.
In an experimental case, the model has been applied to sensor networks to automate and unify their provenance capture even as they grow \cite{5478496}.
To aid in the adoption of the OPM, the framework Karma2 was developed to assist integrating provenance capture into scientific workflows \cite{simmhan2010karma2}.
It reduces the amount of modifications required to adopt the OPM through web services and, more importantly, integrates into scientific workflows.
With the magnitude of data collection endeavors, it is no longer feasible for scientists to stay close to the data and must take a more abstract view of their data collection activities.
Scientific workflows provides this high level view of complex data collection, curation, and analysis \cite{Casati1996}.
The value then of integrating provenance capture and workflow design is that lineage planning can then take place at a high level of scientific work.
This gives insight into how different parts of the workflow fit together and how new exploratory expansions may occur.

Following the OPM, PROV is a W3C recommendation that deliniates a method to express data provenance with semantic technologies that has been accepted as a World Wide Web Consortium (W3C) Recommendation \cite{Belhajjame2013} \cite{Belhajjame2013c} \cite{Groth2013}.
The recommendation uses a conceptual model relating activities, agents, and entities together to describe data production lineage \cite{Belhajjame2013a} \cite{Nies2013} \cite{Missier2013}.
Intended as a high level abstraction, it describes data as entities that are generated by activities enacted by agents.
This basic relationship is very powerful in its ability to describe data production activities.
The expression of the conceptual model occurs through the PROV Ontology (PROV-O), which can be conveyed through various resource description languages \cite{Belhajjame2013b} \cite{Moreau2013a} \cite{Moreau2013b}.
The ontology is further formalized into a functional notation for easier human consumption \cite{Cheney2013} \cite{Cheney2013a}.
One particular strength that has contributed to the adoption of PROV is its ability to link into other ontlogies, making it easier for existing semantically enriched data sets to adopt PROV \cite{Miles2013} \cite{Moreau2013}.
Like the OPM, a framework has also been developed to alleviate workflow integration through Komadu \cite{Suriarachchi_2015}.
The framework improves over its predecessor, Karma, by no longer utilizing global context identifiers that were no necessarily shared throughout the workflow.

PROV has played a significant contribution in maintaining the quality and reproducibility of datasets and reporting in the National Climate Assessment (NCA) \cite{Ma2014191}.
This implication signifies that there is an increased likelihood of adoption through other scientific fields as a result of this reporting.
The Global Change Information System, which houses the data used to generate the NCA, uses PROV to meticulously track the generation of its artifacts and results as they are used in the report \cite{Tilmes2012}.
This means that not only does the data have a traceable lineage to verify quality, but the content of documents can have the same verifiability \cite{Ma2014}. 

\subsection{Changelogs}

Changelogs, sometimes called patch notes, are artifacts resulting from the versioning process often found in major software projects.
They document the changes made within the system and seek to explain, in human language, the motivations behind changes \cite{uel1037}.
The logs provide significant utility to both users and producers as it can serve as both documentation and tutorials.
Many users will often refer to the patch notes in order to decide how to adapt to changes made to the system they use, either data or software.
Meanwhile, changelogs aid producers through team transitions by keeping a history of decisions made to improve the project.
This is particularly evident in the realm of open-source projects as developers can contribute without having been part of the original development team.
The need for documentation to bring new programmers up to speed for a project drives the ability to keep the project alive.

Open source projects have much more consistent adoption of changelogs than data sets, possibly resulting from complex code techniques emerging earlier than large data methods.
These logs provide a great source of value to developers as they can be used to give insight to the health of a software project \cite{German03automatingthe}.
These projects have a tendency to die rather quickly after initial enthusiasm and with the rather low overhead cost to start new open-source projects, some automated methods of determining the progress of a project is needed.
It would give insight into the maturity of a project's development team as well as the likelihood that team members will correct errors within the code.
However, readability proves to remain a significant hurdle as current development change logs contain solely human readable text.
While machines may still be significantly removed from the ability to comprehend the impact of changes made to a data set or software code, they are currently opaquely blocked from consuming any of the content within logs more than understanding they contain text.
The transition between different versions of large datasets is then left largely up to the human user's ability to understand and process the modifications mentioned within the change log.

As mentioned previously, changelogs also allow developers to link bugs and errors with their corrections in new versions of the code \cite{Chen:2004:OCL:990374.990391}.
This gives feedback to the user community that corrections have been addressed as well as ensuring that modifications to the code base are driven by improving the project.
It also has the added benefit of creating a system that can be used to link the introduction of new features with the emergence of new bugs \cite{6132954}.
The resulting discoveries help reveal patterns of development and prevent further occurrences of problematic code.
Therefore, providing an machine consumable changelog would accelerate and assist in navigating through dataset changes and error corrections.

\subsection{RDFa}

In order address the human readability of data change logs, this project considers the use of the Resource Description Framework in Attributes (RDFa) framework \cite{Adida2015}.
The benefits of embedding RDFa into change logs is twofold.
First, the change log would need to be marked up in HTML in order to accept RDFa.
As a result, the log would also become available online and thus, more openly accessible to data users.
This would allow data users to better determine personally how a change applies to their specific application.
Large companies such as Google have already begun making endeavors in equipping their web crawlers to consume structured data such as RDFa from web pages.
Second, the simple application of RDFa attributes encodes the entries within the change log in a format consumable by machines \cite{Herman2015}.
RDFa has already had significant success in adoption across a variety of web publication platform and eases the search for their content \cite{Bizer2013}.
In these applications, however, the developers use RDFa to describe the content on the page, to indicate a string is actually a name for example.
This project endeavors to use RDFa to embed an RDF graph into the web page instead, and therefore, the data becomes captured instead of described.
The language does have the ability to transform into RDF, but the slight nuance between intended use means that a more complicated deployment of the attributes will be required.
Using a previously established standard eases the adoption of encoding required to communication change information to autonomous systems.

\section{Provenance Distance}

Understanding provenance and workflows only provide only a portion of the view into a data set's lineage landscape.
The workflow provides an understanding of how a data set fits into the bigger picture of data analysis and the provenance gives a method to reproduce the data set for data quality purposes.
As such, workflow and provenance describe data sets in a very flat and static manner, allowing for prospective reasoning as to how it may respond to changes made by the data producers.
However, this places the burden of determining the magnitude of change in quality, as changes in provenance mean changes in quality, on the data producer.
Consider again that data quality is subjective with respect to the data consumer's usage, and the difficulty in determining the significance of a new version becomes apparent.
With increasing complexity, data workflows have developed in such a way that even subtle changes have serious implications for other parts of the workflow \cite{TILMES2011548}.
The responsibility of determining and communicating the magnitude of data alterations falls to versioning systems.

A very rudimentary way to communicate change distance uses the version number of the data set.
Returning to discussing the dot-decimal notation often used to number versions, version numbering follows a hierarchical method of systematically counting the changes made to a data set or system based on the perceived magnitude of the change.
Generally, this causes no problems, but when a modification rests close to the border between classes of change, researchers often struggle in determining how to classify such a change.
These cases become challenging since some users will experience larger perturbations resulting from the change than others.
As a result, using fewer well-established categories avoids this problem, but it loses many details in resolution of the extent to which the data set may change.
In addition, there is no standardization as to what each of the numerals used in a version number represents, and this significantly hinders interoperability between information systems.
Data managers will often discuss whether data sets are qualitatively different enough to warrant incrementing one of the version numerals.
While the dot-decimal method is easy to implement and use, its broad categorization severely impairs its ability to express version information beyond a basic functional extent.

Another approach is following the provenance of two data sets and identifying  differences between the lineages of the two data sets.
The total difference between the data is known as their provenance distance.
This distance measure is very new as the availability of computable provenance has been developed fairly recent.
One endeavor to compute over provenance has shown a marked ability to predict disk usage based on the lineage of a data object \cite{dai2014provenance}.
Efforts have also been made to summarize provenance representations to improve consumption \cite{Ainy:2015:ASD:2806416.2806429}.
While the ability to compute over provenance data has been demonstrated, the comparison of two provenance graphs has yet to be widely studied.

Using PROV to represent provenance data in a semantic model produces an acyclic directed graph with labeled nodes.
As a result, the provenance distance problem reduces to the similarity measurement problem.
When measuring similarity, algorithms determine how far two graphs are from being isomorphic \cite{Cao2013}.
General graphs have similar complexity to determine similarity, but node labeling simplifies this process by providing a method to match nodes together.
Other methods also exist to determine similarity under different conditions such as edits necessary to transform one graph into another  \cite{Gao2010}.
Some methods focus primarily on edge changes \cite{Goddard:1996:DGU:246962.246972}.
This kind of analysis resembles similarity measures employed in determining semantic similarity \cite{Hliaoutakis06informationretrieval}.
The main difference lies in semantic similarity comparing the distance between two concepts within a graph as opposed to the distances between the graphs themselves.
However, it does reveal that using semantic graphs can have incredible impact in extracting implied relations between data they store.

There already exist methods which compare workflows based on quality criteria that leverages provenance to bound quality of service \cite{2015:CAA:2778374.2778504}.
However, these procedures focus primarily on quick retrieval and efficient storage instead of leveraging the latent information accessed by reasoning across data set versions \cite{tan2004research}.
The distance measures previously mentioned rely solely on provenance graphs to compute results, but this is obviously insufficient.
When considering the provenance of a data object, methods only consider the activities and entities that took an active role in the production of it.
A new version of an object has a familial relationship with its previous versions, but in most cases, they do not take an active role in its generation.
For this reason, detailed change information falls outside of provenance's scope and it can be seen in PROV using a single relation to link different versions of a data object.
Without detailed change information, determining the difference between two data objects in a metric beyond broad strokes becomes difficult if not impossible.

This is not to say that provenance becomes useless in computing change distance, but it largely serves as an indicator than a measure.
If there is any difference in provenance, then something must have changed.
For example, if workflow uses a new script to generate a data set, changes can be expected in this new data set.
However, what this script does differently than the old one requires a more detailed understanding and description than lineage can provide  \cite{Bose:2005:LRS:1057977.1057978}.
Additionally, if no changes were made to the script, but new data was produced, it likely indicates that some inputs have changed.
The ramifications for the resulting data set will be difficult to determine without understanding how the original inputs have changed.
Only knowing that they have changes is insufficient.
Being able to understand the extent that modifications to data or workflows impact the results greatly improve a producer's ability to generate high quality data.

\section{Data Versioning Operations}

Architecture has a principle that says form follows function, but, for data, form equals function.
As a result, data has as many different forms as it has functions.
Biological experiments often use data within cyclical data workflows where outputs are immediately fed back into new experiments \cite{Tagger2005}.
Even though the goal of the experiment is the final data set, all the intermediary data sets provide significant value in reaching the goal.
Libraries store data about their collections in large databases where both old and new versions of literature need to be maintained \cite{Wiil:2000:RDH:338407.338517}.
Some data exist in such a highly constrained environment that it must be managed at near the hardware level \cite{Flouris04clotho:transparent}.
The struggle no longer becomes generating data, but instead, fitting the data into a format that users find useful and can consume.

The challenge of data versioning systems is to provide a unifying environment that can handle the plethora of forms and functions of its data.
At its core, versioning systems only need to concern themselves with three operations: addition, deletion, and modification.
Most literature surveys do not realize the significance of this commonality as this means that versioning methods can be described by delineating how each operation is approached by a system \cite{Tagger2005} \cite{rohtua}.
Data addition generally constitutes the least complicated versioning operation because it interacts the least with pre-existing data.
However, new data does share context with pre-existing data and provides a method of measuring data set growth.
Since data sets no longer have to be used in its entirety and can be freely subsetted, a data set's complexity increases significantly with its growth.
Every new file added to a data set doubles the number of available subsets.

Data deletion, however, has a more philosophical difference between systems.
From the perspective of a versioning specialist, data should never be deleted since knowing why data was excluded is as important as knowing why data was included.
The software versioning manager GIT uses a method of compressing older data to conserve space without deleting the data \cite{Chacon:2009:PG:1618548}.
Pragmatically, this is not always possible due, generally, to the physical constraints of storage space.
In high energy physics, observational data often cannot be re-collected due to cost, and as a result, poor quality data cannot be re-processed or replaced \cite{Cavanaugh2002}.
The decision in this document is to use the term invalidation when referring to data removal operations as it implies that whether permanently deleted or not, there exists a more valid alternative.

Data modification encompasses the most involved data versioning operation.
As a result, it often comprises a majority of the description of a data versioning service.
In truth, data modification can be summarized as the invalidation of an instance of a data object - which can be a file, a record, or anything in a data set - followed by the addition of a new instance of that data object.
However, this kind of operation is used so often to fix errors and update data sets that it is considered a unique operation.
Modification owes its complexity to interacting with both pre-existing data from the invalidation stage and new data from the addition stage.
However, this compound relationship fully contextualizes the relationship the operation has in relating the old data and the new data.
In some cases, this only provides forward or backwards references between data versions, but having both gives users context for data's current state and update to new data \cite{Klein01ontologyversioning}.

Due to the ubiquity of the data addition, invalidation, and modification operations in versioning systems, the conceptual versioning model presented in Chapter 3 centers around capturing the relationships established by each of the operations.
While other functions exist commonly in versioning systems such as object locking to prevent simultaneous conflicting changes, viewing to see the version an object belongs to, and branching to allow distributed modifications, these functions comprise the space of utility operations that support the three core processes.

\subsection{Types of Change}

The study of versioning operations further breaks down into categorization of change types data sets may undergo.
While the meaning of operations are fairly easy to understand, not all changes have the same impact.
As mentioned previously, version numbering separates perturbations into categories based on the impact the producer believes it has on the project.
In this project, changes are categorized into scientific, technical, and lexical changes.
The granularity of the categorization does not consider the magnitude of change within the individual values stored by a data set as actual values vary depending on application and domain.
Focusing on a more abstract representation of kinds of change allows for a better understanding of its impact while not being too precise to be domain specific.

Scientific changes comprise the family of changes which have the greatest impact on a project or data set.
It indicates that modifications have been made to the most fundamental methods used to create a data set.
These can include changes to algorithms used or sampling methods which may require a change in how users consume the new data.
These changes have the largest implications for data consumers as it can have serious consequences for the soundness of their results.
However, these kinds of changes is not always caught on the production end of data generation.
While very large modifications can easily be determined to produce a scientific change, more subtle changes or interactions can also have larger ramifications, and data producers may initially view this as a technical change due to data quality's subjectivity.
Technical impacts do not change the underlying science of the data, but impose a large enough change as to warrant notice.
Structure alteration and unit conversions count as technical changes since the dataset now needs to be consumed differently but remains valid for use.
In one of the data sets used by this projects, concentration units were originally reported in parts per million and then in cc isotope ratios.
This would constitute a technical change since the data presents the same scientific measurements but in a different manner.
Lexical changes belie the transformations that can best be described as corrections.
Filling in previously missing values or fix erroneous values may be lexical changes.
While they have the smallest impact on results and conclusions, these changes can allow computations to be performed when previously missing data discouraged such behavior.

The exact category that a particular change falls into can be controversial.
The decision to change concentration units from parts per million to milligrams per milliliter poses a Technical change for a data producer.
However, for a data consumer, the change may be viewed as a Scientific change as it invalidates the methods they had previously used.
This conflict in view illustrates the data consumer-producer dynamic.
In general, data producers are in control of the methods of versioning, but data consumers determine the classification of a data change.
Producers tend to use versioning systems to ensure data quality of service through audits and recovery tools \cite{Cavanaugh2002}.
Meanwhile, a consumer will analyze the historical changes and determine the impact this may have to their data use.
As a result, this means that data versioning systems must communicate a dynamic view of the changes in a system contextualized by the user of that data.

\section{Thesis Statement}

The growth of innovative data capturing and storage technologies has led to new challenges in properly tracking the changes stored data sets undergo.
Researchers store data in a variety of formats, from documents to databases, but since the growth in project scope, many have relied on versioning methods from software management technologies to track their data's evolution.
However, these techniques fail to properly capture the changes data undergoes because they do not take into account the impact that a data's structure has on its function.
In order to maintain data quality, producers use provenance meta-data capture the series of activities and agents involved in generating a data entity.
Emergent technologies and frameworks have been developed to digitally capture this information including OPM and PROV.
This data can be used to give insight into the magnitude of change a data entity has undergone by comparing the differences in provenance between the two entities, but this can only be done in broad strokes without more detailed change data.

By looking at the versioning transactions operating on a data set, data consumers can have a better idea as to the extent their data changes.
This thesis document develops a concept model to formally characterize the activities and relationships among data objects when transitioning between versions.
The model would improve current discussion on data versioning by providing a common understanding of terms and activities, changing versioning from a rule of thumb chore to a valuable research activity.
It then demonstrates the model's utility by applying it to tabular spreadsheets, and then generalizes it to other contexts.
In addition, this contribution addresses in more detail the problem of measuring change distance, which using only provenance data could not accomplish, by utilizing the graph structure of linked data.
Through this process, it creates and makes accessible machine readable change logs which allows for clearer deductions when comparing the extent of changes.
This enriches the version documentation process without developing new artifacts, but now allows for the introduction of automation into part of the data auditing workflow.
In addition, the method reinforces the need for dynamic, publicly available change information as data becomes more flexible to accommodate research in more diverse fields.


%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
