%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                                                 %
%                            CHAPTER ONE                          %
%                                                                 %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{INTRODUCTION}

\section{Definitions of Version} \label{sec:def}

Using the term `version' in the vernacular has become so pervasive that few documents formally define it.
Barkstrom describes versions as \textbf{homogeneous groupings} used to control, ``production volatility induced by changes in algorithms and coefficients as result of validation and reprocessing," \cite{Barkstrom2003}.
The \textbf{groupings} he mentions are a method of separating data objects such that they have similar scientific or technical properties.
In order to determine when these properties have changed, he leverages the \gls{nasa} Earth Science workflow model shown in Figure \ref{NASALevels}.
\begin{figure}
	\centering
	\includegraphics[scale=0.35]{figures/NASALevels.png}
	\caption[National Aeronautics and Space Administration Earth Science organizes its data into three levels depending on how much the data has been aggregated and processed from the original sensor measurements.]{National Aeronautics Space Administration Earth Science organizes its data into three levels depending on how much the data has been aggregated and processed from the original sensor measurements. Figure 1 from \cite{Barkstrom2003}}
	\label{NASALevels}
\end{figure}
The model describes the formal stages of processing to turn a raw remote sensing signal from satellite instruments into global aggregate summaries \cite{Barkstrom2003}.
The workflow model, therefore, describes a data object's \gls{provenance}, the information about how a piece of data or thing was created and used to determine its quality, reliability or trustworthiness \cite{Moreau2013c}, not the version differences.
Provenance provides the mechanism to explain differences between \textbf{groupings} exposed by versioning activities.
Understanding the workflow model reveals that changes to either the algorithms or parameter files will force a change in the resulting data, creating a new version of the output data.
Each time the provenance changes, the levels take on a layer cake appearance as shown in Figure \ref{NASALayers}. 
\begin{figure}
	\centering
	\includegraphics[scale=1]{figures/VersionLayerDiagram2.png}
	\caption[Versions Layered on the National Aeronautics and Space Administration Earth Science workflow model]{National Aeronautics and Space Administration Earth Science workflow model gains layers as changes to provenance induces version groupings to expand.  The new layer is homogeneous with other layers in the level, forming a grouping.  Level 0 cannot gain more layers because manipulating the source readings would invalidate the data.}
	\label{NASALayers}
\end{figure}
Unlike provenance, version information describes relations not along the workflow, but in an orthogonal direction because change passes through the layers.

The conflation can also be seen in \gls{linked} provenance models, Web of Data resources which describe the \gls{provenance} relating data sets, where version concepts and links are included into the ontology.
The \gls{prov} is the \gls{w3c} recommended method to document \gls{provenance}, connecting versions using the property \textit{prov:derivation}.
The \textit{prov:derivation}, covered more in Section \ref{sec:prov}, is defined as, ``a transformation of an entity into another, an update of an entity resulting in a new one, or the construction of a new entity based on a pre-existing entity," \cite{Lebo2013}.
The \gls{prov} definition establishes the existence of at least two distinct objects as a necessary condition for versioning.
An additional result of the \gls{prov} definition is that an object cannot be a version of itself.

Another definition comes from Tagger at University College London on versioning of biology data in which versions are a, ``semantically meaningful snapshot of a design object (an artifact) at a point in time," \cite{Tagger2005}.
He, unfortunately, does not further clarify what he means by semantically meaningful since the requirements to be meaningful varies based on the application.
He likely means that the design object must be complete or whole, interpretable in purpose by itself and not a subset.
The design object is used in the context of database objects, referring to an incomplete final product, and unifies the versions as their primary subject, capturing the object's state over the course of its design.

The \gls{ifla} formed a study group in February 1997 to, ``produce a framework that would provide a clear, precisely stated, and commonly shared understanding of what it is that the bibliographic record aims to provide information about, and what it is that we expect the record to achieve in
terms of answering user needs" \cite{frbr}.
A common problem in library sciences is the organization and documentation of multiple editions of a book or multiple translations of a book in a standardized manner.
A primary result of the group meeting is a standardized vocabulary to discuss distinct iterations of a literary work which follows strong parallels in data versioning.
The \gls{frbr} avoids the terms \textbf{edition} and \textbf{version} since ``those terms are neither clearly defined nor uniformly applied" \cite{frbr}.
Instead, they use the terms: \textbf{work}, \textbf{expression}, and \textbf{manifestation}.
A \textbf{work} refers to the abstract concept of a creative or artistic idea.
\textbf{Expressions} are then different forms of that particular \textbf{work}, embodying the most similar term to the previous definitions of versions.
A \textbf{manifestation} is the physical embodiment of an \textbf{expression}.
These three terms and their hierarchy establish a repeating theme throughout other versioning works.
The breakdown of terms identifies a need to differentiate between the granularity or scale of changes as differences in \textbf{works} are more significant than differences in \textbf{expressions}.
Models focused on versioning rather than provenance will also have a hierarchical structure as covered in Section \ref{sec:models}.

Combining these myriad of definitions, the Barkstrom definition provides the mechanism to detect and explain the source of a version.
The Tagger and \gls{prov} definitions explain the purpose of a version to expose differences between distinct objects.
The \gls{frbr} definition contextualizes a version as a more granular \textbf{expression} of a larger \textbf{work}.
The working definition in this dissertation of a \gls{version} is `an \textbf{expression} of a \textbf{work} which exists in comparison to another object and communicates the extent to which it diverges from that object as a result of provenance changes'.

Although each definition disagrees on the form a version object takes on, all but \gls{prov} derivation agree that a version belongs to a larger collection of objects implementing a more abstract, ideal representation.
Provenance provides the information necessary to explain ``semantically meaningful" for the Tagger definition as \textit{prov:Derivation} captures when a data object diverges into a new object.

\section{Why Versioning is Important}

\begin{quotation}
	``If scientific data production were easy, instruments would
	have stable calibrations and validation activities would discover no need for
	corrections that vary with time. Unfortunately, validation invariably shows that
	instrument calibrations drift and that algorithms need a better physical basis." \cite{Barkstrom2003}
\end{quotation}

Anyone who has used an iPhone or owned a video game console understands the basics of versioning.
Companies brand sequential devices to indicate improvements in performance or capabilities.
Basic numerical sequencing has given rise to a plethora of versioning systems used widely across a landscape of software and data.
At the very core, versioning systems are a means of communication.
Data producers communicate to consumers how much the producer has changed the data.
The producer can also communicate through time using logs and other documentation.
In order to clearly communicate changes, ideas must be clearly formalized and defined and vice versa.
Versioning systems help scientific workflows avoid losing work by managing transitions and changes while in operation \cite{Casati1996}.
Versioning systems provide necessary documentation which informs the transition to new methods and procedures \cite{Wiil:2000:RDH:338407.338517}.
Versioning systems provide accountability for the value of a project's data set when considering an agency's continued funding \cite{Cavanaugh2002}.
The natural evolution of versioning systems, however, have given rise to formal architecture operating on top of very informal concepts.
Disagreements between the informal concepts propagate through the formal architecture, causing breakdowns in interoperability between heterogeneous systems, especially when users are not aware of biases in informal versioning concepts \cite{Baker2009}.
In this dissertation, we identify gaps in versioning practices which result from tradition and develop a data model to more completely capture the interactions involved in versioning.

\section{Change Capture Completeness}
Provenance - definition
The ontologies declare relation and influence but do not make comparisons
The ontologies are missing addition and invalidation changes.

\section{Change Log Performance}
Working definition for change log performance is content per storage space where content is an entry for a specific change.
Buneman finds that data provenance consumes storage space on the order of the original data to a fifth after compression.
Encoding increases the storage space used to convey a change so performance must decrease when encoded with linked data.
The expected impact to performance should be no more than 50\% as provenance data and the original data is double the original size.


\section{Change Distance Precision}
Dot-decimal identifier tradition only increments a number in the series by one with each new version.
Change at identifier interval summarizes the amount of change in the data
Number of changes can range from 1 or more.
Counting the actual changes or dfferences allows more precision in reporting the amount of change between versions.

\section{Accuracy of Data Set Change Rate}
Version identifiers enable sequential ordering.
Differences between identifiers communicate change across versions, but not across time.
Git and others show that versions can be published at an arbitrary rate to control the stability of a data set or project.
Variable number of changes are contained in each version.
Using the change distance instead of the version metric increases the accuracy of change rate reporting.

\section{The Versioning Use Case} \label{sec:usecase}

The research questions addressed by this thesis revolve around the check and retrieval stages, steps 3 and 4a, of the flow in the Versioning Use Case shown in Table \ref{table:UseCase}.
\begin{table}
	\caption{Versioning Use Case Table}
	\label{table:UseCase}
	\centering
	\begin{tabular}{|L{14cm}|}
		\hline
		\textbf{Use Case Name}: New Version Publication \& Retrieval\\
		\hline
		\textbf{Goal}: Record a new version of a data set and provide it to a data consumer.\\
		\hline
		\textbf{Summary}: The Producer creates a new version of their data and must record it to the Versioning System while providing the Consumer with the data\\
		\hline
		\textbf{Actors}: Producer, Consumer\\
		\hline
		\textbf{Preconditions}: Producer has supplied some data to the Versioning System.   Consumer has retrieved the data from the Versioning System.\\
		\hline
		\textbf{Triggers}: Producer provides a different version of the data to the Versioning System.\\
		\hline
		\textbf{Basic Flow}: 
		\begin{enumerate}
			\item Producer places the next version into the Versioning System.
			\item Producer may repeat the previous action 0 or more times.
			\item Consumer checks the data to see if there are changes.
			\item If there are pertinent changes:
			\begin{enumerate}
				\item Consumer retrieves the updated data.
			\end{enumerate}
		\end{enumerate}\\
		\hline
		\textbf{Alternate Flow}:
		\begin{enumerate}
			\item Producer places the next version into the Versioning System.
			\item Producer notifies Consumer that an alternate version is available.
			\item Consumer retrieves the updated data.
		\end{enumerate}\\
		\hline
		\textbf{Post Conditions}: Producer has made the alternate version available.  Consumer possesses the pertinent newly available version.\\
		\hline
	\end{tabular}
\end{table}
The goal of the Versioning Use Case is to allow the Producer to communicate changes of data tracked by the versioning system to the Consumer.
The use case begins when a data producer wishes to distribute the next \gls{version} of a data set.
Note the use of next \gls{version} instead of new \gls{version} to signify that the Versioning Use Case also includes situations where the data is not new or describes concurrent events.
The use case features the Producer and Consumer as actors using the versioning system as an intermediary.
The actors do not follow separate use cases because each actor relies on the actions of the other actor to contextualize the actions in the use case.

The basic flow of the use case is seen in Figure \ref{UCD1} where the Producer logs a version into the versioning system, and the Consumer retrieves that data version.
\begin{figure}
	\centering
	\includegraphics[scale=1]{figures/UC_Diagram1.png}
	\caption[Versioning Use Case Basic Flow]{Basic Flow of the Versioning Use Case.  The Producer adds a data set to be tracked by the versioning system.  After the Consumer retrieves the data, the Producer makes a series of updates and corrections to the original data.  The Consumer then returns to determine if there are changes to pertinent data files.  The Consumer then updates as necessary.}
	\label{UCD1}
\end{figure}
At a later point in time, the Producer can add any number of corrections to the versioning system.
The repeated corrections reflect a practice of passively logging additional changes.
At some final time after 0 or more corrections, the Consumer returns to check the versioning system for any changes to the data set the Consumer retrieved.
If there are changes to parts pertinent to the Consumer, the Consumer retrieves the corrected data.
In the interaction, the Producer only passively provides additional versions of the data while the responsibility of remaining up-to-date lies with the Consumer.
The ability of the  versioning system to communicate to the Consumer that the data has changed determines whether the use case succeeds or fails for the data consumer.
The relationship creates a Producer/Consumer dynamic which influences the performance of the versioning system.

The alternate flow in Figure \ref{UCD2} requires additional information from the Consumer which is a means of notification.
\begin{figure}
	\centering
	\includegraphics[scale=1]{figures/UC_Diagram2.png}
	\caption[Versioning Use Case Alternate Flow]{In the alternate flow, the authority to determine necessary updates lies with the Producer.  After correcting data tracked by the versioning system, the Producer notifies the Consumer to update the Consumer's data.}
	\label{UCD2}
\end{figure}
The flow begins the same as the basic flow, but after a single correction, the Producer notifies the Consumer that the data has changed.
The notification causes the Consumer to come and retrieve the updated data set.
The alternate flow poses a few problems in that the Producer must now manage notification information, but the Producer must also notify all consumers of the data which may cause scalability issues.
Notice that in both flows, the Producer possesses the authority to determine change through data publication.
The Consumer only takes from the versioning system except in step three of the basic flow.
It is only when the Consumer checks the \glspl{version} in the system or is notified by the Producer that the Consumer can contextualize changes to the retrieved data set.

\subsection{Research Question 1: What has changed?}

The primary research question addressed in the following chapters pertains to step three in the basic flow of the use case.
In order to execute step three, the Consumer must have a means of determining whether the data set is different from the one the Consumer currently possesses.
In a large number of software projects, especially open-source projects, \glspl{log} document for the Producer changes to the code base and communicate alterations to the Consumer.
Very few data sets provide detailed \glspl{log} along with \glspl{version} in the versioning system.
Either general descriptions of changes are made or new documentation explaining usage is provided, requiring the data consumer to manually acclimate to changes.

The prevailing approach to addressing Research Question 1 is that automating the generation of \glspl{log} will allow the addition of \gls{linked} into \gls{log} content.
To encode changes as \gls{linked}, I developed a versioning ontology called \gls{vo}.
The additional data allows data consumers the ability to search, filter, and otherwise interact with change information.
Different standards exist across various national agencies, but few fundamentals have been established on what needs to be included and how the information is to be captured or presented.
I determine whether a \gls{linked} versioning model would provide a standardized basis for discussing version change as well as allow tools to be developed to assist in step three for large data sets of which Consumers may only use a part.

\subsection{Research Question 2: How much has changed?}

A follow-up query to Research Question 1 is that once the changes have been determined, Consumers need to evaluate the impact of the changes.
As previously mentioned, Producers sequentially label \glspl{version} to communicate change, but many have adopted the practice of using decimals or dots to imply fractional changes, forming a dot-decimal identifier.
The authority to determine the magnitude of change lies with the Producer since the label must be applied at publication.
Version counts, defined in Chapter \ref{ch:graph}, provide a very bland description of the changes in a version by aggregating a range of changes under a sequential label.
Using \gls{vo}, the changes can be broken down into \gls{AIM} classifications which provide a quantitative method to determine \gls{changedist}.
I test whether the AIM change counts would tie change metrics back to data differences rather than to versions.
I determine whether the AIM changes also provide a means to compare the difference between the amount of change declared by the Producer and quantity of change as seen by various Consumers.

\subsection{Research Question 3: How fast does the data change?}

Once a standardized metric for determining \gls{changedist} has been determined, assessing change rate logically follows.
Up to this point, evaluating the differences between versions has been discussed without respect to time.
Notice from the Versioning Use Case that the Producer has the ability to control the rate of version release by determining when to push corrections into the versioning system, meaning the time between \glspl{version} is not required to be consistent.
Variable release times allows the Producer to control the data volatility, the likelihood of a data set to change, by aggregating changes over time.
Without considering time, \gls{changedist} across \glspl{version} misrepresents the actual change rate of a data set.
Looking at just the version publication rate also misrepresents the actual change rate of a data set since total change can vary between \glspl{version}.
I hypothesize that version analysis must factor in time when comparing change rates between \glspl{version}.
The change rate distribution of each \gls{AIM} \gls{change} follows a distribution significantly different from the version publication rate.

\section{Limitations}
VersOn cannot compute changes of a data set with only one version.
VersOn was modeled on data sets with months or years separating versions or with concurrent versions containing on the order of thousands of entries only.
VersOn does not scale well as the number of attributes needed to uniquely identify an entry increases.

\section{Hypothesis Statement}

The work in this dissertation tests five hypotheses.
\begin{enumerate}
	\item Using the versioning model \gls{vo} to expose \glspl{log} as \gls{linked} standardizes versioning publication in \glspl{log}.
	\item \gls{AIM} counts computes a more accurate \gls{changedist} than the current practice by capturing data differences rather than the sequential labeling common to many versioning traditions.  \gls{AIM} provides a method to compare within a particular kind of change---\gls{add}, \gls{invalidate}, or \gls{modify}---through subclassing concepts within \gls{vo}.
	\item \gls{AIM} counts provide a method to compare the amount of change as seen by various data consumers.
	\item Computing \gls{changedist} without time misrepresents the actual change rate.
	\item The distribution change rate of data sets is significantly different from the publication rate of versions.
\end{enumerate}

\section{Contributions}

%Add chapter number and references.

In Chapter \ref{ch:model}, I present my first contribution, the model forming the basis of \gls{vo} which is used to instantiate \glspl{vergraph}.
\Glspl{vergraph} capture differences between objects, not the course used to create a data object, differentiating themselves from provenance graphs.
The \gls{vergraph} enables my second contribution, a structured process to compute and publish \gls{linked} \glspl{log} to test Hypothesis 1, covered in Chapter \ref{ch:changelog}.
The contribution eases consuming very lengthy logs, which data sets often produce, as well as enabling searchability and discoverability of \glspl{change} affecting the \gls{version}.
My third contribution, discussed in Chapter \ref{ch:graph}, is using a \gls{vergraph} to provide a quantitative metric for determining \gls{changedist}, a necessary component to answer Hypothesis 2.
Differing methods between data producers and consumers in computing \gls{changedist} highlights disagreements in the prodcuer/consumer dynamic which is demonstrated while evaluating Hypothesis 3.
A fourth contribution is a comparison between version publication and change rates, found in Chapter \ref{Volatility}, to determine the propriety of versions as proxies for change, used to test Hypothesis 4 and 5.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
