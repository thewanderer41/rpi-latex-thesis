\chapter{DISCUSSION \& CONCLUSION}

\section{Introduction}

Based on the results presented in Chapter \ref{ch:model}, the versioning model more completely captures the changes occurring between versions by preserving version continuity, capturing Addition, Invalidation, and Modification (AIM) changes, and directly connection attributes with versions.
The results in Chapter \ref{ch:changelog} showed that the impact of encoding VersOn into change logs exceeds the expected 50\% decrease in performance.
VersOn produced results in Chapter \ref{ch:graph} indicating an increase in change distance precision using the ontology by enumerating the changes between dot-decimal identifiers.
The results from Chapter \ref{Volatility} highlights the increased accuracy in capturing data set change rates after factoring time into the change distances.

\section{Versioning Graph Completeness}

Three requirements were identified to obtain completeness in a versioning graph:
\begin{enumerate}
	\item Captures \acrfull{AIM} change modeling
	\item Links attributes to versions
	\item Maintains version continuity
\end{enumerate}
A provenance based model, discussed in Chapter \ref{ch:model}, makes the new version responsible for all differences in a version change, partially satisfied the second and third requirements.
Old attributes, as seen in Figure \ref{DiscardedFig}, invalidated by the new version were incorrectly linked to the appropriate version, but the changing attribute shared between Version 1 and Version 2 is correctly linked.
For example, entry Tur030 in Figure \ref{NobleGraph2} would be associated with Version 3 of the Noble Gas database even though the attribute no longer appears in the version.
The changing attribute, however, does not provide correct version continuity when a third version is added.
All versions become associated with a singular attribute which makes the proper ordering of the pre- and post-values ambiguous.

A log based model, shown in Figure \ref{DiscardedFig2}, satisfied the change modeling requirement, but fails to meet the second and third requirements.
Because all the changes are associated with the log, attributes are not linked to the original data versions, and there is no way to preserve version continuity through the model.
Placing the log model between versions in a hybrid approach, seen in Figure \ref{DiscardedFig3}, re-establishes continuity between versions, where a third version can be added without violating or confusing the concepts in the previous versions.
The attribute to version connection remains absent in the hybrid construction since attributes are still associated with the Log concept instead of the versions.

A fully connected model satisfied the attribute to version connectivity requirement and the version continuity requirement.
The attributes were directly associated with the appropriate version and continuity through versions was achieved through the relation between attributes.
As changes link attributes across versions, the attributes imply a continuity because each attribute must be a part of a version.
The relationship is the same one seen in the \textbf{modification} change in Figure \ref{CopperGraphVerGraph} and Figure \ref{NobleGraph2}.
The fully connected model does not satisfy the requirement to distinguish between the \gls{AIM} changes.
The formulation has a single change, but cannot model missing attributes due to addition and invalidation.

The final model, implemented as VersOn, separates the fully connected model into three formulations to satisfy all three requirements.
The addition and invalidation formulations remove an attribute to successfully model the associated \gls{AIM} changes, and uses the version-change relation to maintain version continuity.
The modification formulation maintains continuity through versions by the implied relation across attributes in the model.
Attributes in each of the forms are directly associated with the appropriate versions to satisfy the second requirement.

\section{Effects on Change Log Performance}

Prior work from Buneman \cite{Buneman} and the close relation to provenance data suggested that encoding linked-data change information into a change log would decrease performance of the log by more than 50\%.
The findings in Section \ref{sec:CLA} show that change data does not consistently utilize space on the order of the original data set.
In Table \ref{table:Ng_changelog_table1}, the un-encoded change log is a little over double the size of the original data file.
The RDFa change log was then more than ten times the size of the text-only log, meaning that the performance of the change log has decreased by at least 90\%.
The JSON-LD change log was even larger, over twenty times the size of the text-only log, resulting in a 95\% decrease to performance.
In both of the encoded logs, the impact to performance is significantly greater than the expect 50\%.

The impact to performance continues to be seen in Table \ref{table:Ng_changelog_table2} and Table \ref{table:Cu_changelog_table1} where the text-only change logs are smaller than the original data file.
The RDFa change logs have consistently 90\% reductions to performance and the JSON-LD change logs reduce performance by approximately 95\%.
Looking at Table \ref{table:Ng_turtle} and Table \ref{table:Cu_change}, notice that the version changes are dominated by \textbf{modification} changes.
A comparison in the Copper Minerals Database show that \textbf{addition} and \textbf{invalidation} changes gain large compression benefits from row and column summarization, but \textbf{modification} cannot summarize the changes it captures.
Combining the dominance and compression observations, \textbf{modifications} play a significant role in failure to meet change log performance expectations.
The performance reduction does not entirely restrict adoption since a single script, as seen in Appendix \ref{app:gcmd}, can dynamically generate change logs for a number of versions due to standardization.
The responsibility of log storage does not entirely lie with the data producer, but may be adopted by the data consumer as needed or dynamically generated.

A finding through the encoding process is that RDFa introduces a number of challenges to creating a linked-data change log compared to JSON-LD.
The RDFa functions by annotating visible data in the document, but VersOn utilizes very little of the visible content, meaning RDFa forces some change content to be adjusted to conform with the XML or HTML structure.
JSON-LD frees the VersOn from the strict structure of the visible change log document, allowing the intended expression of the model.
The trade-off to achieve more accurate version model content results in increased storage space.

\section{Increases to Change Distance Precision}

The GCMD Keyword Governance and Community Guide Document defines the requirements for labeling major and minor releases using dot-decimal identifiers \cite{gcmd_gov}.
Figure \ref{GCMDC1} shows that VersOn is able to provide more precise counts, showing that major version changes differed by three hundred to five hundred \textbf{additions}.
VersOn, additionally, characterizes GCMD Keywords as an addition dominated data set.
In each of the minor version changes between GCMD Keyword versions, VersOn is able to more precisely distinguish between the number of changes separating the versions than a single increment of the minor version number.
In the MBVL data set, VersOn is able to support the comparison within the \gls{AIM} changes, enabling increased precision without any modifications to the base ontology.
The additional precision using taxonomic ranks was achieved through sub-classing the \gls{AIM} change concepts in the ontology.
Increased precision in change distance from VersOn enables more detailed comparisons between versions rather than the categorical comparisons currently enabled by dot-decimal identifiers.
VersOn change counts enables the computation of other attributes more precisely such as a data set's rate of change.

\section{Increases to Change Rate Accuracy}

After redistributing GCMD Keyword changes across time, the change rates organized into three clusters as seen in Figure \ref{GCMDPlot1Cluster}.
In the third cluster, versions were released at a rapid pace, less than a month apart.
The changes implemented by the Versions 8.2 to 8.4 were found to originate from years prior to the release of the immediately previous version.
The changes are not restricted to the valid time frame of the immediately previous version.
Using the starting time of the change rather than the preceeding version, the change rates of Cluster 3 match the rates in Cluster 2.
Using the change time instead of the version publication time to compute the VersOn change rate increases the accuracy of the change rate measurements.

The EOL collects a large number of small-sized data sets, around thirteen hundred, but just over a thousand of the data sets have only one version, making the data sets un-versionable by VersOn.
Of the remaining 180 data sets, distribution of version publication rates was compared to each of the \gls{AIM} change rates using the Kolmogorov-Smirnov test.
The test compares the cumulative distribution functions of each distribution to find the largest different to determine likelihood both functions come from the same distribution.
The version publication rate does not match the distribution of any of the change rates, and because VersOn captures the changes between versions, VersOn change rates represented the more accurate change rate.
VersOn allows data consumers to better understand the actual data set rate of change, enabling connected systems to form better expectations of changing data's effects.

\section{Producer/Consumer Versioning Dynamic}

In the diagrams in Figure \ref{UCD1} and Figure \ref{UCD2}, notice that all the information going into the Versioning System originates from the Producer.
Even in the one interaction originating from the Consumer, the Consumer does not input any information into the system.
In the use case, the Producer poses sole authority over defining the change information.
Looking at dot-decimal identifiers, the Producer once again posses sole authority on determining the interval to increment.
Because the identifier is also a data set label, a Producer must finalize the identifier prior to publication, meaning the Consumer must use the label regardless of the amount of pertinent change.

In the GCMD Keywords version change from 8.4.1 to 8.5 in Figure \ref{gcmd_85}, we saw that the amount of change varies greatly depending on the Consumer utilization of the data set.
In the Bridged method, the version change is \gls{modify} dominant, a vastly different behavior than all prior version changes.
The Standard method used URIs to match attributes, but the method resulted in a total replacement of the data set.
We know that GCMD Keywords used a third method of assessing the change because the Standard and Bridged methods constitute a full keyword release, requiring a new major version label.
VersOn allows data Consumers to self-assess the amount of change created by a version change rather than being forced to use the assessment of the producer at the time of publication.
VersOn also increases the transparency of the methods that the Producer used to make a conclusion of the change distance, resulting in a particular version identifier.

The need for versioning method transparency becomes especially apparent in Table \ref{table:GCMD_metric}.
The impact assessments and VersOn both use addition, invalidation, and modifications to count the number of changes made in the resulting version.
The assessments, however, are based on requests made to the keywords group while VersOn used URIs and taxonomy structure to determine counts.
The difference in methods result in conflicting assessments to the amount of change and effected keywords.
The implementation of VersOn clarifies the distinction by making the mapping method between versions to assess change explicit and visible.

In the Earth Observing Laboratory, the versioning method is not apparent the Consumer.
Each data set has very few versions with minor version name differences, but a majority of the data sets are entirely replace or at least use entirely different filenames as seen in the linear trend between \textbf{additions} and \textbf{invalidations} in Figure \ref{EOL_AIM}.
The pattern of behavior between versions is hidden or inconsistent between data sets collected by the laboratory.

\section{New Versioning Nomenclature}
Analysis of versioned data sets has revealed three types of data, dependent on the way in which versions are released: single, periodic, and intermittent.
Single version data sets contain data which cannot be replicated or in which modification would entirely invalidate the data.
High energy physics, previously mentioned, and surveillance data fit within this category.
The data sets in this category will usually only experience additions and invalidations since scientists cannot change the data.

Periodic data sets exhibit version releases at regular intervals in time.  Large data collections usually exhibit a regular behavior when they follow a periodic data collection scheme.
The ARM data center releases data at daily intervals, meaning new versions every day.
The reasons that ARM data sets are not overloaded with version numbers is that some operations, in this case new files, are masked to increase the pertinence of each version designation.
The problem that masking additions causes is the actual amount of change within the data set over time also becomes masked.
The data set then appears to be intermittent when it actually undergoes periodic changes.  As seen in GCMD Keywords and EOL, changes are not necessarily evenly distributed among versions.  The changes, as a result, are also not evenly distributed across time.  As mentioned with distributed versioning methods, periodic version releases can be used to control the volatility of a data set by collecting many changes over time before publication.  Periodic data sets expend version identifiers very quickly since they must release a version even if few significant changes have occurred.
Periodic data sets have the benefit of being accurate within a bounded time frame.

The final type of data set follows intermittent versioning which is characterized by releasing versions as appropriate or as necessary.  The data sets are not bound by an established release schedules.  In the intermittent category falls GCMD Keywords, the Copper data set, and the Noble Gas data set.  Irregular version releases allows data managers the freedom to reduce the number of versions necessary to manage the data set.  When data managers wait too long to release a new version, the number of changes in a single transition can overwhelm methods to track modifications to the data as seen in the Noble Gas data set.  Since intermittent versions are not released based on time, it is very important that versions are released based on some other quantitative measure of change.  Failing to do so invites unclear or worse arbitrary distinction between versions.  GCMD Keywords define clear requirements for major and minor version releases, but the governance document does not explain the requirements for sub-minor versions which occasionally appear in the keyword repository.

Each data set type can additionally be sub-divided into two categories based on the observations made with the AIM model: Add dominant and Modify dominant.  In the data sets currently studied, none exhibit behavior suggesting an Invalidate dominant data set.  A data set is either Add or Modify dominant when a majority of versions have a majority of either Adds or Modifies.  Add dominance indicates that the data is primarily growing while Modify dominance shows that a data setâ€™s coverage is primarily stable but occasionally undergoes adjustments.  The GCMD Keywords is an example of an Add dominant data set since all its version transitions are comprised of new concepts.  The Noble Gas data set shows modify dominance.

\section{Conclusion}

The work in this thesis has demonstrated that VersOn has been successfully applied to a very diverse set of data sources, ranging from static tabular databases to taxonomic trees.
The model we developed, instantiated in VersOn, improved change capture of linked-data models by satisfying the three requirements of completeness.
Data change logs were made available as linked-data as a result of the improved completeness.
Our process to encode VersOn into change logs was not able to bound change log performance reduction to at most 50\% because \textbf{modifications} were not summarized over rows or columns.
The results showed that the ability to summarize change plays a key role in managing the effects of version model encoding on change log performance.
We were able, through VersOn enabled change counts, to increase the precision of change measurements compared to dot-decimal identifiers by enumerating the changes between each identifier.
The work with MBVL demonstrated encoding domain knowledge around VersOn enabled more precise classification of changes which are pertinent to the domain.
Increased precision led to standardized comparison between versions and the more precise computation of other change attributes like the change rate.
Our incorporation of time into the VersOn change counts demonstrated an increased accuracy of data set change rate assessment by disconnecting changes from the distribution across versions.
Accurately understanding the data set change rate is important to evaluating the needs of versioning systems to manage changes in a data set.
Our work in this thesis additionally found that VersOn improved the transparency of versioning methods used between the data Producer and the data Consumer by revealing the methods used to assess change and label versions.
The standardized change log and versioning graph generation using VersOn enabled the data Consumer to assess data set change independent from Producer assessments by providing tools, shown in the Appendices, to compute change counts and generate change logs from the client side of the versioning system.
Our analysis of change counts and version publication rates revealed a pattern of classification between different data sets based on the rate of change and the dominant change type which may, in future work, reveal particular patterns or needs in versioning.